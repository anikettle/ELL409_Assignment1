{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nFor prior assumpsion, we take the medically known values and some rough assumptions on our behalf-\\nFor y=0\\n    Age - 30 (Younger people usually have higher tendency)\\n    Rest Blood Pressure - 120 (According to my understanding of what I obtained from the internet)\\n    Cholestrol Level - 162.5 (125 to 200 - we bluntly take the average, therefore, = 162.5)\\n    \\nFor y=1\\n    Age - 60 \\n    Rest Blood Pressure - 150 \\n    Cholestrol Level - 200 \\n\\nVariance we randomly take as 1 across all features and covariance as 0 (We will vary it and see the results)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "'''\n",
    "For prior assumpsion, we take the medically known values and some rough assumptions on our behalf-\n",
    "For y=0\n",
    "    Age - 30 (Younger people usually have higher tendency)\n",
    "    Rest Blood Pressure - 120 (According to my understanding of what I obtained from the internet)\n",
    "    Cholestrol Level - 162.5 (125 to 200 - we bluntly take the average, therefore, = 162.5)\n",
    "    \n",
    "For y=1\n",
    "    Age - 60 \n",
    "    Rest Blood Pressure - 150 \n",
    "    Cholestrol Level - 200 \n",
    "\n",
    "Variance we randomly take as 1 across all features and covariance as 0 (We will vary it and see the results)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(split_percent = 70):\n",
    "    dataset = pd.read_csv('E:\\ELL_project\\problem1\\health_data.csv')\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "    X = dataset.iloc[:,:-1].values\n",
    "    y = dataset.iloc[:,-1:].values\n",
    "    datasize = X.shape[0]\n",
    "\n",
    "    split_point = split_percent//10\n",
    "\n",
    "    X_train = X[:(datasize*split_point)//10,:]\n",
    "    y_train = y[:(datasize*split_point)//10,:]\n",
    "    X_test = X[(datasize*split_point)//10:,:]\n",
    "    y_test = y[(datasize*split_point)//10:,:]\n",
    "\n",
    "    return(X_train,X_test,y_train,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(X_train,X_test):\n",
    "    X_mean = np.sum(X_train,axis=0)\n",
    "    X_var = np.sqrt(np.sum(np.square(X_train - X_mean), axis=0))\n",
    "\n",
    "    X_train_feat_scaled = (X_train - X_mean ) / X_var\n",
    "    X_test_feat_scaled = (X_test - X_mean) / X_var\n",
    "\n",
    "    return (X_mean,X_var, X_train_feat_scaled,X_test_feat_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sep_01(X_train,y_train):\n",
    "    x_0_train = X_train[y_train[:,0]==0]\n",
    "    x_1_train = X_train[y_train[:,0]==1]\n",
    "    return(x_0_train,x_1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_var(x):\n",
    "    siz = x.shape[0]\n",
    "    feat = x.shape[1]\n",
    "    mean = np.sum(x,axis=0)/siz\n",
    "    mean = mean.reshape((1,feat))\n",
    "    dif = x-mean\n",
    "    covariance_matrix = np.dot(np.transpose(dif),dif) / siz\n",
    "\n",
    "    return(mean,covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_meu_var(m1=30,m2=120,m3=162.5,v1=1,v2=1,v3=1,m11=60,m12=150,m13=200,v11=1,v12=1,v13=1):\n",
    "    num_feat = 3\n",
    "    prior_mean_0 = np.array([m1,m2,m3])\n",
    "    prior_variance_0 = np.zeros((num_feat,num_feat))\n",
    "    prior_variance_0[0,0] = v1\n",
    "    prior_variance_0[1,1] = v2\n",
    "    prior_variance_0[2,2] = v3\n",
    "\n",
    "    # prior_variance_0 = np.array([v1,v2,v3])\n",
    "\n",
    "    prior_mean_1 = np.array([m11,m12,m13])\n",
    "    prior_variance_1 = np.zeros((num_feat,num_feat))\n",
    "    prior_variance_1[0,0] = v11\n",
    "    prior_variance_1[1,1] = v12\n",
    "    prior_variance_1[2,2] = v13\n",
    "    # prior_variance_1 = np.array([v11,v12,v13])\n",
    "\n",
    "    prior_mean_0 = prior_mean_0.reshape((1,num_feat))\n",
    "    prior_mean_1 = prior_mean_1.reshape((1,num_feat))\n",
    "\n",
    "    # print('prior_meu_shape_0: ', prior_mean_0.shape)\n",
    "    # print('prior_meu_shape_1: ', prior_mean_1.shape)\n",
    "\n",
    "    prior_vals = [0,0,0,0]\n",
    "    prior_vals = [prior_mean_0,prior_variance_0,prior_mean_1,prior_variance_1]\n",
    "    return(prior_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_n(meu0,meu,cov0,cov,data_size):\n",
    "    # Now, we will consider L as the left most value, M as the middle one and R as the right one in the formula for meu        # and cov matrix\n",
    "    l = cov0\n",
    "    m = np.linalg.inv(cov0 + (cov/data_size))\n",
    "    r = cov/data_size\n",
    "\n",
    "    meu_n = np.dot(np.dot(l,m),np.transpose(meu)) + np.dot(np.dot(r,m),np.transpose(meu0))\n",
    "    meu_n = np.transpose(meu_n)\n",
    "    # print('sfsgs',meu_n)\n",
    "    cov_n = np.dot(np.dot(l,m),r)\n",
    "\n",
    "    return (meu_n,cov_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_var_data(X_train,y_train,prior_vals):\n",
    "    x_0,x_1 = sep_01(X_train,y_train)\n",
    "    meu_0,covar_0 = mean_var(x_0)\n",
    "    meu_1,covar_1 = mean_var(x_1)\n",
    "    # print(meu_1,'verbe')\n",
    "\n",
    "    prior_mean_var = prior_vals\n",
    "    \n",
    "    prior_mean_0 = prior_mean_var[0]\n",
    "    prior_covar_0 = prior_mean_var[1]\n",
    "    prior_mean_1 = prior_mean_var[2]\n",
    "    prior_covar_1 = prior_mean_var[3]\n",
    "\n",
    "    size_0 = x_0.shape[0]\n",
    "    meu_n_0,cov_n_0 = ret_n(prior_mean_0,meu_0,prior_covar_0,covar_0,size_0)\n",
    "    size_1 = x_1.shape[0]\n",
    "    meu_n_1,cov_n_1 = ret_n(prior_mean_1,meu_1,prior_covar_1,covar_1,size_1)\n",
    "\n",
    "    return(meu_n_0,cov_n_0, meu_n_1,cov_n_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test(X_test,meu_n_0,cov_n_0,meu_n_1,cov_n_1):\n",
    "\n",
    "    num_of_feat = cov_n_0.shape[0]\n",
    "    det_0 = np.linalg.det(cov_n_0)\n",
    "    det_1 = np.linalg.det(cov_n_1)\n",
    "\n",
    "    p0 = 1/math.pow((2*math.pi),num_of_feat/2)\n",
    "    p0 = p0 / (det_0**0.5)\n",
    "    p1 = 1/math.pow((2*math.pi),num_of_feat/2)\n",
    "    p1 = p1 / (det_1**0.5)\n",
    "\n",
    "    m0 = np.transpose(meu_n_0)\n",
    "    m1 = np.transpose(meu_n_1)\n",
    "\n",
    "    print('m0: ',m0.shape)\n",
    "\n",
    "    exp_0 = np.exp(-0.5 * (np.dot(X_test-meu_n_0,np.linalg.inv(cov_n_0)))*(X_test-meu_n_0))\n",
    "    exp_1 = np.exp(-0.5 * (np.dot(X_test-meu_n_1,np.linalg.inv(cov_n_1)))*(X_test-meu_n_1))\n",
    "\n",
    "    # exp_0 = np.exp(-0.5 * np.sum(np.multiply(np.dot(X_test-meu_n_0,np.linalg.inv(cov_n_0)),(X_test-meu_n_0)),axis=1).reshape(-1,1))\n",
    "    # exp_1 = np.exp(-0.5 * np.sum(np.multiply(np.dot(X_test-meu_n_1,np.linalg.inv(cov_n_1)),(X_test-meu_n_1)),axis=1).reshape(-1,1))\n",
    "\n",
    "    print(exp_0.shape)\n",
    "\n",
    "    # exp_0 = np.exp(-0.5 * (np.dot(np.dot(X_test-m0,np.linalg.inv(cov_n_0)),np.transpose(X_test-m0))))\n",
    "    # exp_1 = np.exp(-0.5 * (np.dot(np.dot(X_test-m1,np.linalg.inv(cov_n_1)),np.transpose(X_test-m1))))\n",
    "\n",
    "    # print('Xtest: ',X_test.shape)\n",
    "    # print('meu_n_0: ', meu_n_0.shape)\n",
    "    # print('cov_n_0: ', cov_n_0.shape)\n",
    "    # print('meu_n_1: ', meu_n_1.shape)\n",
    "    # print('cov_n_1: ', cov_n_1.shape)\n",
    "    \n",
    "    # print(exp_0.shape)\n",
    "\n",
    "    return ( exp_0/p0 , exp_1/p1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicter(prob_X_0,prob_X_1,threshold=1):\n",
    "    y_pred = np.zeros(prob_X_0.shape)\n",
    "    y_pred[prob_X_1[:,:]/prob_X_0[:,:] > threshold] = 1\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metrics(y_pred_thresh, y_test):\n",
    "\n",
    "    test_size = y_test.shape[0]\n",
    "\n",
    "    tp = np.sum((y_pred_thresh+y_test)==2 , axis=0)[0]\n",
    "    tn = np.sum(y_pred_thresh==y_test , axis=0)[0] - tp\n",
    "    fp = np.sum(y_pred_thresh , axis=0)[0]-tp\n",
    "    fn = test_size-tp-tn-fp\n",
    "\n",
    "\n",
    "    print('tp: {} , tn: {} , fp: {} , fn: {}'.format(tp,tn,fp,fn))\n",
    "\n",
    "    acc = (tp+tn)/test_size\n",
    "    prec = (tp)/(tp+fp)\n",
    "    recl = (tp)/(tp+fn)\n",
    "    f1 = 2*prec*recl/(prec+recl)\n",
    "\n",
    "    print('Accuracy: {}'.format( acc  ))\n",
    "    print('Precision: {}'.format( prec  ))\n",
    "    print('Recall: {}'.format( recl  ))\n",
    "    print('F1 score: {}'.format( f1  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_estimator(threshold=1,split_percent=70):\n",
    "    X_train,X_test,y_train,y_test = import_dataset(split_percent)\n",
    "    X_train_0 , X_train_1 = sep_01(X_train,y_train)\n",
    "    prior_vals = gen_meu_var(m1=30,m2=120,m3=162.5,v1=1,v2=1,v3=1,m11=60,m12=150,m13=200,v11=1,v12=1,v13=1)\n",
    "    meu_n_0,cov_n_0, meu_n_1,cov_n_1 = mean_var_data(X_train,y_train,prior_vals)\n",
    "    # print(meu_n_0)\n",
    "    prob_0,prob_1 = eval_on_test(X_test,meu_n_0,cov_n_0,meu_n_1,cov_n_1)\n",
    "    print(prob_0.shape,prob_1.shape)\n",
    "    y_pred = predicter(prob_0,prob_1,threshold)\n",
    "    accuracy_metrics(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "m0:  (3, 1)\n(210, 3)\n(210, 3) (210, 3)\ntp: 73 , tn: 94 , fp: 25.0 , fn: 18.0\nAccuracy: 0.7952380952380952\nPrecision: 0.7448979591836735\nRecall: 0.8021978021978022\nF1 score: 0.7724867724867726\n"
     ]
    }
   ],
   "source": [
    "map_estimator(threshold = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}