{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_list_of_data(bpt = 1000):\n",
    "    data_dir = 'E:\\ELL_Project\\problem3\\\\'\n",
    "    lis_folders = os.listdir(data_dir+'.')[:-1]\n",
    "    X = []\n",
    "    cur_class = 0\n",
    "    for folder in lis_folders:\n",
    "        i=0\n",
    "        lis_files = os.listdir(data_dir+folder+'\\.')\n",
    "        im = []\n",
    "        for files in lis_files:\n",
    "            i+=1\n",
    "            if(i==bpt+1):\n",
    "                break\n",
    "            img = '\\\\'.join([data_dir,folder,files])\n",
    "            image = plt.imread(img)\n",
    "            im.append(image.flatten().reshape(-1,1))\n",
    "        im_np = np.array(im)[:,:,0]\n",
    "        im_np = np.append(im_np,cur_class*np.ones((im_np.shape[0],1)),axis=1)\n",
    "        cur_class+=1\n",
    "        X.append(im_np)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensional_shrinking_pca(X,n_components=2):\n",
    "    pca = decomposition.PCA(n_components)\n",
    "    X_train = X[0]\n",
    "    for x in X[1:]: \n",
    "        X_train = np.append(X_train,x,axis=0)\n",
    "    X_pca = X_train\n",
    "    np.random.shuffle(X_pca)\n",
    "    pca.fit(X_pca[:,:-1])\n",
    "    X_pca_ret = np.zeros((X_train.shape[0],n_components))\n",
    "    y_pca_ret = X_pca[:,-1].reshape(-1,1)\n",
    "    X_pca_ret = pca.transform(X_pca[:,:-1])\n",
    "\n",
    "    return (X_pca_ret,y_pca_ret)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y,num_feats):\n",
    "    y = y.reshape(-1,1)\n",
    "    print(y)\n",
    "    ret = np.zeros((y.shape[0],num_feats))\n",
    "    rows = np.arange(y.shape[0])\n",
    "    # print(y)\n",
    "    # print(rows)\n",
    "    ret[ rows , y[:,0].astype(int)] = 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(X,y,split_percent = 70):\n",
    "    \n",
    "    datasize = X.shape[0]\n",
    "\n",
    "    split_point = split_percent//10\n",
    "\n",
    "    X_train = X[:(datasize*split_point)//10,:]\n",
    "    y_train = y[:(datasize*split_point)//10,:]\n",
    "    X_test = X[(datasize*split_point)//10:,:]\n",
    "    y_test = y[(datasize*split_point)//10:,:]\n",
    "\n",
    "    return(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_hyp(X,theta):\n",
    "    return (np.dot(X,theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_regularizer(alpha,theta):\n",
    "    a = np.zeros(theta.shape)\n",
    "    return(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_reg(alpha,theta):\n",
    "    reg_loss = alpha*theta\n",
    "    reg_grad = alpha\n",
    "    return(reg_loss,reg_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_reg(alpha,theta):\n",
    "    reg_loss = alpha * np.square(theta)\n",
    "    reg_grad = 2 * alpha * theta\n",
    "    return(reg_loss,reg_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_net_reg(lambda1,lambda2,theta):\n",
    "    a1,a2 = l1_reg(lambda1,theta)\n",
    "    b1,b2 = l2_reg(lambda2,theta)\n",
    "    return( a1+b1 , a2+b2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(X,theta,y,hypothesis):\n",
    "    siz = y.shape[0]\n",
    "    h = hypothesis(X,theta)\n",
    "    diff = h-y\n",
    "    mse = ( np.sum(np.square(diff),axis=0))[0] / siz\n",
    "    gradient = np.dot(np.transpose(X),diff) / siz\n",
    "\n",
    "    return(h,mse,gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_loss(X,theta,y,hypothesis):\n",
    "    siz = y.shape[0]\n",
    "    num_feat = y.shape[1]\n",
    "\n",
    "    h = hypothesis(X,theta)\n",
    "    diff = h-y\n",
    "    diff_sign = np.ones((siz,1))\n",
    "    diff_sign[diff[:,0]<0] = -1\n",
    "\n",
    "    mae = np.sum(np.abs(h),axis=0)[0] / siz\n",
    "    gradient = np.sum(diff_sign*X,axis=0) / siz\n",
    "\n",
    "    return(h,mae,gradient)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(X,theta,y,hypothesis):\n",
    "    siz = y.shape[0]\n",
    "\n",
    "    h = hypothesis(X,theta)\n",
    "    h_comp = 1-h\n",
    "    diff = h - y\n",
    "\n",
    "    ce = np.sum(-y*log(h)-(1-y)*log(1-h),axis=0) [0] / siz\n",
    "    gradient = np.dot(np.transpose(X),diff) / siz\n",
    "\n",
    "    return(h,ce,gradient)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradDesc(X,y,theta,hypothesis,loss_function,regularizer,alpha):\n",
    "    h,loss,gradient = loss_function(X,theta,y,hypothesis)\n",
    "    reg_loss,reg_grad = regularizer(alpha,theta)\n",
    "    loss += reg_loss\n",
    "    gradient += reg_grad\n",
    "\n",
    "    return(loss,gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linReg(X,y,iter=100,alpha=0.01,batchSize=32,n_components=2,classes=6):\n",
    "    theta = np.random.random((X.shape[1],y.shape[1]))\n",
    "    datasize = X.shape[0]\n",
    "    for cl in range(classes):\n",
    "        y_class = y[:,cl:cl+1]\n",
    "        loss_epoch,_ = gradDesc(X,y_class,theta[:,cl:cl+1],lin_hyp,mse_loss,l2_reg,alpha)\n",
    "        for i in range(iter+1):\n",
    "            if((i)%100000==0):\n",
    "                print('Loss for {} iterations: {}'.format(i,np.sum(loss_epoch,axis=0)[0]))\n",
    "            fro = 0 \n",
    "            loss_epoch = 0\n",
    "            while(True):\n",
    "                to = min(fro+batchSize,datasize)\n",
    "                l,theta_grad = gradDesc(X[fro:to,:],y_class[fro:to,:],theta[:,cl:cl+1],lin_hyp,mse_loss,l2_reg,alpha)\n",
    "                loss_epoch += l\n",
    "                theta[:,cl:cl+1] -= (alpha*theta_grad)\n",
    "                fro = to\n",
    "\n",
    "                if(to>=datasize):\n",
    "                    break\n",
    "    \n",
    "    return (theta,loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(X_train):\n",
    "    training_size = X_train.shape[0]\n",
    "    X_mean = np.sum(X_train,axis=0) / training_size\n",
    "    X_var = np.sqrt(np.sum((np.square(X_train-X_mean)),axis=0)/training_size)\n",
    "    X_train_reg = (X_train - X_mean) / X_var\n",
    "    return (X_mean,X_var,X_train_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_feat(X,X_var,degree=2):\n",
    "    num_feats = X.shape[1]\n",
    "    num_vals = X.shape[0]\n",
    "    X_prev = X\n",
    "    X_cross = np.append(np.ones(X.shape),X,axis=1)\n",
    "    if(degree==0):\n",
    "        return(np.ones(X.shape))\n",
    "    if(degree==1):\n",
    "        return(X_cross)\n",
    "    for i in range(num_feats):\n",
    "        for j in range(i+1,num_feats):\n",
    "            X_cross = np.append( X_cross ,  np.multiply ( X[:,i:i+1] , X[:,j:j+1] ) , axis=1 )\n",
    "    for i in range(2,degree):\n",
    "        X_prev = np.multiply(X_prev,X)\n",
    "        X_prev = X_prev / X_var\n",
    "        X_cross = np.append(X_cross,X_prev,axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "    return ( X_cross )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyReg(X,y,iter=100,alpha=0.01,batchSize=32,degree=2,n_classes = 6):\n",
    "    X_mean,X_var,X_norm = feature_scaling(X)\n",
    "    X_cross = poly_feat(X_norm,X_var,degree)\n",
    "    train_size = X_cross.shape[0]\n",
    "    X_cross = np.append(np.ones((train_size,1)),X_cross,axis=1)\n",
    "    y_onehot = one_hot(y,n_classes)\n",
    "    opt, tl = linReg(X_cross,y_onehot,iter=iter,alpha=alpha,batchSize=batchSize)\n",
    "    return ( X_mean, X_var, opt , tl )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicter(X_mean, X_var, X_test, opt_theta,degree=2):\n",
    "    X_test_norm = (X_test - X_mean) / X_var\n",
    "    X_test_cross = poly_feat(X_test_norm, X_var,degree)\n",
    "    test_size = X_test_cross.shape[0]\n",
    "    X_test_cross = np.append( np.ones((test_size,1)) , X_test_cross , axis=1)\n",
    "\n",
    "    # print(X_test_cross.shape)\n",
    "    # print(opt_theta.shape)\n",
    "    print(degree)\n",
    "    print(X_test_cross)\n",
    "    print(opt_theta)\n",
    "    y_pred = np.dot(X_test_cross, opt_theta)\n",
    "    y_pred_norm = y_pred / (np.sum(y_pred,axis=1).reshape(-1,1))\n",
    "    y_pred_class = y_pred.argmax(1)\n",
    "\n",
    "    return y_pred_class,y_pred_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(X_mean,X_var,X_test,y_test,opt_theta,degree=2,n_classes=6):\n",
    "    y_pred_class,y_pred_norm = predicter(X_mean, X_var, X_test, opt_theta, degree)\n",
    "    y_test_one_hot = one_hot(y_test,n_classes)\n",
    "    y_pred_one_hot = one_hot(y_pred_class,n_classes)\n",
    "    for i in range(n_classes):\n",
    "        print('class {} .......................................'.format(i))\n",
    "        accuracy_metrics_classif(y_pred_one_hot[:,i:i+1],y_test_one_hot[:,i:i+1])\n",
    "\n",
    "\n",
    "def accuracy_metrics_classif(y_pred,y_test):\n",
    "    \n",
    "    loss_y = y_pred - y_test\n",
    "\n",
    "    test_size = y_pred.shape[0]\n",
    "    total_loss_y = np.dot(np.ones((1,test_size)),np.square(loss_y))[0,0] / test_size\n",
    "\n",
    "    # print(total_loss_y)\n",
    "\n",
    "    y_pred_thresh = y_pred>=0.5\n",
    "\n",
    "    tp = np.sum((y_pred_thresh+y_test)==2 , axis=0)[0]\n",
    "    tn = np.sum(y_pred_thresh==y_test , axis=0)[0] - tp\n",
    "    fp = np.sum(y_pred_thresh , axis=0)[0]-tp\n",
    "    fn = test_size-tp-tn-fp\n",
    "\n",
    "\n",
    "    print('tp: {} , tn: {} , fp: {} , fn: {}'.format(tp,tn,fp,fn))\n",
    "\n",
    "    acc = (tp+tn)/test_size\n",
    "    prec = (tp)/(tp+fp)\n",
    "    recl = (tp)/(tp+fn)\n",
    "    f1 = 2*prec*recl/(prec+recl)\n",
    "\n",
    "    print('Accuracy: {}'.format( acc  ))\n",
    "    print('Precision: {}'.format( prec  ))\n",
    "    print('Recall: {}'.format( recl  ))\n",
    "    print('F1 score: {}'.format( f1  ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy_metric_classif(X_mean,X_var,X_train,X_test,y_train,opt_theta,degree=2):\n",
    "    print('Train Accuracy')\n",
    "    accuracy_metrics_classif(X_mean,X_var,X_train,y_train,opt_theta,degree)\n",
    "\n",
    "    print('..............................................')\n",
    "\n",
    "    print('Test Accuracy')\n",
    "    accuracy_metrics_classif(X_mean,X_var,X_test,y_test,opt_theta,degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(y_pred,y_test):\n",
    "    return(np.sum(np.square(y_pred-y_test),axis=0)[0] / y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(y_test,y_pred):\n",
    "    siz = y_test.shape[0]\n",
    "    x_plot = np.arange(0, siz, 1)\n",
    "    # print(x_plot[:10])\n",
    "    plt.plot(x_plot, y_pred, 'g')\n",
    "    plt.plot(x_plot, y_test, 'b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_run(X_train,X_test,y_train,y_test,degree = 3, plot = False,alpha=0.0001,n_classes = 6):\n",
    "    # X_mean,X_var, X_train_feat_scaled = feature_scaling(X_train , X_test)\n",
    "    train_datasize = X_train.shape[0]\n",
    "    X_mean, X_var, opt_theta, train_loss = polyReg(X_train,y_train,50000,alpha,train_datasize,degree,n_classes)\n",
    "    y_pred = predicter(X_mean, X_var, X_test, opt_theta,degree)\n",
    "    acc(X_mean,X_var,X_test,y_test,opt_theta,degree=2)\n",
    "    if(plot):\n",
    "        plotter(y_pred,y_test)\n",
    "        y_pred_train = predicter(X_mean, X_var, X_train, opt_theta, degree)\n",
    "        plotter(y_pred_train,y_train)\n",
    "    return(X_mean, X_var,opt_theta, degree,y_pred)\n",
    "# print_accuracy_metric_classif(X_mean,X_var,X_train,X_test,y_train,opt_theta,degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[5.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]]\n",
      "Loss for 0 iterations: 12.312046344208262\n",
      "Loss for 0 iterations: 37.463176481791194\n",
      "Loss for 0 iterations: 12.628114727056596\n",
      "Loss for 0 iterations: 6.48036816477775\n",
      "Loss for 0 iterations: 16.988981117761362\n",
      "Loss for 0 iterations: 6.838157167178759\n",
      "2\n",
      "[[ 1.          1.          1.          0.38874609 -0.75534752 -0.29363839]\n",
      " [ 1.          1.          1.          1.32369848  0.58061591  0.7685604 ]\n",
      " [ 1.          1.          1.          0.43280281 -0.89736802 -0.3883834 ]\n",
      " ...\n",
      " [ 1.          1.          1.         -0.68517883  2.01547844 -1.38096316]\n",
      " [ 1.          1.          1.         -0.90286907  0.33883577 -0.30592434]\n",
      " [ 1.          1.          1.         -0.88429459  0.11863847 -0.10491136]]\n",
      "[[-0.11539099  0.09671404 -0.06747165 -0.25347331 -0.38768052 -0.25231554]\n",
      " [-0.30508943 -0.06676747  0.14636532  0.04273028  0.11674201 -0.08927983]\n",
      " [ 0.58542359  0.13263856  0.09041844  0.38196717  0.43866276  0.50639272]\n",
      " [ 0.06964125 -0.21441309  0.14002135  0.24662017 -0.09275231 -0.12193681]\n",
      " [-0.12377476 -0.0914265  -0.17578263  0.18669698  0.23864445  0.02186529]\n",
      " [-0.06109041  0.21202095 -0.17578472  0.27091658 -0.15607973  0.00328261]]\n",
      "2\n",
      "[[ 1.          1.          1.          0.38874609 -0.75534752 -0.29363839]\n",
      " [ 1.          1.          1.          1.32369848  0.58061591  0.7685604 ]\n",
      " [ 1.          1.          1.          0.43280281 -0.89736802 -0.3883834 ]\n",
      " ...\n",
      " [ 1.          1.          1.         -0.68517883  2.01547844 -1.38096316]\n",
      " [ 1.          1.          1.         -0.90286907  0.33883577 -0.30592434]\n",
      " [ 1.          1.          1.         -0.88429459  0.11863847 -0.10491136]]\n",
      "[[-0.11539099  0.09671404 -0.06747165 -0.25347331 -0.38768052 -0.25231554]\n",
      " [-0.30508943 -0.06676747  0.14636532  0.04273028  0.11674201 -0.08927983]\n",
      " [ 0.58542359  0.13263856  0.09041844  0.38196717  0.43866276  0.50639272]\n",
      " [ 0.06964125 -0.21441309  0.14002135  0.24662017 -0.09275231 -0.12193681]\n",
      " [-0.12377476 -0.0914265  -0.17578263  0.18669698  0.23864445  0.02186529]\n",
      " [-0.06109041  0.21202095 -0.17578472  0.27091658 -0.15607973  0.00328261]]\n",
      "[[0.]\n",
      " [3.]\n",
      " [0.]\n",
      " ...\n",
      " [4.]\n",
      " [5.]\n",
      " [5.]]\n",
      "[[2]\n",
      " [3]\n",
      " [2]\n",
      " ...\n",
      " [4]\n",
      " [4]\n",
      " [1]]\n",
      "class 0 .......................................\n",
      "tp: 0 , tn: 1494 , fp: 0 , fn: 306\n",
      "Accuracy: 0.83\n",
      "Precision: nan\n",
      "Recall: 0.0\n",
      "F1 score: nan\n",
      "class 1 .......................................\n",
      "tp: 322 , tn: 1305 , fp: 173 , fn: 0\n",
      "Accuracy: 0.9038888888888889\n",
      "Precision: 0.6505050505050505\n",
      "Recall: 1.0\n",
      "F1 score: 0.7882496940024479\n",
      "class 2 .......................................\n",
      "tp: 285 , tn: 1200 , fp: 315 , fn: 0\n",
      "Accuracy: 0.825\n",
      "Precision: 0.475\n",
      "Recall: 1.0\n",
      "F1 score: 0.6440677966101694\n",
      "class 3 .......................................\n",
      "tp: 279 , tn: 1500 , fp: 13 , fn: 8\n",
      "Accuracy: 0.9883333333333333\n",
      "Precision: 0.9554794520547946\n",
      "Recall: 0.9721254355400697\n",
      "F1 score: 0.9637305699481865\n",
      "class 4 .......................................\n",
      "tp: 274 , tn: 1369 , fp: 139 , fn: 18\n",
      "Accuracy: 0.9127777777777778\n",
      "Precision: 0.6634382566585957\n",
      "Recall: 0.9383561643835616\n",
      "F1 score: 0.7773049645390071\n",
      "class 5 .......................................\n",
      "tp: 0 , tn: 1492 , fp: 0 , fn: 308\n",
      "Accuracy: 0.8288888888888889\n",
      "Precision: nan\n",
      "Recall: 0.0\n",
      "F1 score: nan\n"
     ]
    }
   ],
   "source": [
    "n_components = 2\n",
    "n_classes = 6\n",
    "data = return_list_of_data(bpt = 1000)\n",
    "X_pca_ret,y_pca_ret = dimensional_shrinking_pca(data,n_components)\n",
    "X_train,X_test,y_train,y_test = import_dataset(X_pca_ret,y_pca_ret)\n",
    "pow_l,train_l,test_l = [],[],[]\n",
    "# for power in range(0,10):\n",
    "#     X_mean, X_var,opt_theta, degree,y_pred = model_run(X_train,X_test,y_train,y_test,power,False)\n",
    "#     y_pred_train = predicter(X_mean, X_var, X_train, opt_theta, power)\n",
    "#     l_train = calc_loss(y_pred_train,y_train)\n",
    "#     y_pred_test = predicter(X_mean, X_var, X_test, opt_theta, power)\n",
    "#     if power==10:\n",
    "#         print('CHECK',y_pred_test[:15,:])\n",
    "#     l_test = calc_loss(y_pred_test,y_test)\n",
    "#     pow_l.append(power)\n",
    "#     train_l.append(l_train)\n",
    "#     test_l.append(l_test)\n",
    "power=2\n",
    "X_mean, X_var,opt_theta, degree,y_pred = model_run(X_train,X_test,y_train,y_test,power,False)\n",
    "# y_pred_train = predicter(X_mean, X_var, X_train, opt_theta, power)\n",
    "# l_train = calc_loss(y_pred_train,y_train)\n",
    "# y_pred_test = predicter(X_mean, X_var, X_test, opt_theta, power)\n",
    "# l_test = calc_loss(y_pred_test,y_test)\n",
    "# pow_l.append(power)\n",
    "# train_l.append(l_train)\n",
    "# test_l.append(l_test)\n",
    "\n",
    "# plt.plot(pow_l, test_l, 'g')\n",
    "# plt.plot(pow_l, train_l, 'b')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(pow_l, test_l, 'g^')\n",
    "# plt.plot(pow_l, train_l, 'bs')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}