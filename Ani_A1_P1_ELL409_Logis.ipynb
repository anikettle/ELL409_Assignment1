{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ani_A1_P1_ELL409_Lin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxEoxCmkj2_n"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o38f5eQCwZZ0"
      },
      "source": [
        "dataset = pd.read_csv('E:\\ELL_project\\problem1\\health_data.csv')\n",
        "dataset = dataset.sample(frac = 1)\n",
        "X = dataset.iloc[:,:-1].values\n",
        "y = dataset.iloc[:,-1:].values\n",
        "datasize = X.shape[0]\n",
        "X_train = X[:(datasize*7)//10,:]\n",
        "y_train = y[:(datasize*7)//10,:]\n",
        "X_test = X[(datasize*7)//10:,:]\n",
        "y_test = y[(datasize*7)//10:,:]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKVkjhy7x5L-"
      },
      "source": [
        "def sigmoid(X):\n",
        "    ret = 1/(1+np.exp(-X))\n",
        "    return ret"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVnmeL0uwa96"
      },
      "source": [
        "def gradDesc(X,y,theta,loss_fxn):\n",
        "    h  = np.dot(X,theta)\n",
        "    hyp = sigmoid(h)\n",
        "    dsc = X.shape[0]\n",
        "    # print(y.shape)\n",
        "\n",
        "    loss = np.zeros(y.shape)\n",
        "    total_loss = 0\n",
        "    update = np.zeros((X.shape[1],1))\n",
        "\n",
        "    if (loss_fxn=='mse'):\n",
        "        loss = hyp-y\n",
        "        total_loss = np.sum(np.square(loss),axis=0)[0] / dsc\n",
        "        update = np.dot(np.transpose(X),loss)\n",
        "        update /= dsc\n",
        "\n",
        "    elif(loss_fxn=='mae'):\n",
        "        # print('check')\n",
        "        loss = hyp-y\n",
        "        total_loss = np.sum(np.abs(loss),axis=0)[0] / dsc\n",
        "        \n",
        "        loss[loss[:,:]>0] = 1\n",
        "        loss[loss[:,:]<0] = -1\n",
        "\n",
        "        update = np.dot(np.transpose(X),loss)\n",
        "        update /= dsc\n",
        "        \n",
        "\n",
        "    elif(loss_fxn=='ce'):\n",
        "        # print('check')\n",
        "        los = hyp - y\n",
        "        loss = (-y * np.log(hyp)) + (-(1-y) * np.log(1-hyp))\n",
        "        total_loss = np.sum(loss,axis=0)[0] / dsc\n",
        "        update = np.dot(np.transpose(X),los)\n",
        "        update /= dsc\n",
        "        \n",
        "\n",
        "\n",
        "    return(total_loss,update)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [],
      "source": [
        "# n = np.array([1,2,3,4,5,6])\n",
        "# n-=3\n",
        "# n[n[:]>0]=1\n",
        "# n[n[:]<0]=-1\n",
        "# print(n)\n",
        "# print(n[:3]*n[3:])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV3ittIgxs8-"
      },
      "source": [
        "def logisReg(X,y,iter=100,alpha=0.01,batchSize=32,loss_fxn='mse'):\n",
        "    theta = np.random.random((X.shape[1],1))\n",
        "    # print(theta)\n",
        "    datasize = X.shape[0]\n",
        "    loss_epoch = 0\n",
        "    for i in range(iter):\n",
        "        if((i+1)%1000==0 and i>0):\n",
        "            print('Loss for {} iterations: {}'.format(i+1,loss_epoch))\n",
        "        fro = 0\n",
        "        loss_epoch = 0\n",
        "        while(True):\n",
        "            to = min(fro+batchSize,datasize)\n",
        "            l,theta_grad = gradDesc(X[fro:to,:],y[fro:to,:],theta,loss_fxn)\n",
        "            # print(theta_grad)\n",
        "            loss_epoch += l\n",
        "            theta -= (alpha*theta_grad)\n",
        "            fro = to\n",
        "\n",
        "            if(to>=datasize):\n",
        "                break\n",
        "    \n",
        "    return (theta,loss_epoch)\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCI0yp8Vy0B2",
        "outputId": "ac214b82-01e2-4468-c3a7-8a7bdd465e49",
        "tags": []
      },
      "source": [
        "train_datasize = X_train.shape[0]\n",
        "# LOSS FUNCTIONS TO TRY - MSE,MAE,CROSSENTROPY\n",
        "# mse - MSE\n",
        "# mae - MAE\n",
        "# ce - Cross entropy\n",
        "opt_theta, train_loss = logisReg(X_train,y_train,250000,0.000001,train_datasize,'mae')\n",
        "print(opt_theta)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss for 1000 iterations: 0.5795918367346938\n",
            "Loss for 2000 iterations: 0.5795918367346938\n",
            "Loss for 3000 iterations: 0.5795918367346938\n",
            "Loss for 4000 iterations: 0.5795918367346938\n",
            "Loss for 5000 iterations: 0.5795918366994828\n",
            "Loss for 6000 iterations: 0.5795918009561537\n",
            "Loss for 7000 iterations: 0.5795914544035998\n",
            "Loss for 8000 iterations: 0.5795876144163973\n",
            "Loss for 9000 iterations: 0.5795418812996187\n",
            "Loss for 10000 iterations: 0.5789515379005012\n",
            "Loss for 11000 iterations: 0.5708954467860691\n",
            "Loss for 12000 iterations: 0.48476370425968185\n",
            "Loss for 13000 iterations: 0.3312670445420954\n",
            "Loss for 14000 iterations: 0.4124968415883342\n",
            "Loss for 15000 iterations: 0.42040217226375015\n",
            "Loss for 16000 iterations: 0.42041730580950426\n",
            "Loss for 17000 iterations: 0.4204088508315659\n",
            "Loss for 18000 iterations: 0.4204081943096855\n",
            "Loss for 19000 iterations: 0.42040816240647194\n",
            "Loss for 20000 iterations: 0.4204081627528641\n",
            "Loss for 21000 iterations: 0.420408163165168\n",
            "Loss for 22000 iterations: 0.4204081632494158\n",
            "Loss for 23000 iterations: 0.4204081632629901\n",
            "Loss for 24000 iterations: 0.4204081632649831\n",
            "Loss for 25000 iterations: 0.4204081632652622\n",
            "Loss for 26000 iterations: 0.4204081632653003\n",
            "Loss for 27000 iterations: 0.42040816326530533\n",
            "Loss for 28000 iterations: 0.420408163265306\n",
            "Loss for 29000 iterations: 0.4204081632653061\n",
            "Loss for 30000 iterations: 0.4204081632653061\n",
            "Loss for 31000 iterations: 0.4204081632653061\n",
            "Loss for 32000 iterations: 0.4204081632653061\n",
            "Loss for 33000 iterations: 0.4204081632653061\n",
            "Loss for 34000 iterations: 0.4204081632653061\n",
            "Loss for 35000 iterations: 0.4204081632653061\n",
            "Loss for 36000 iterations: 0.4204081632653061\n",
            "Loss for 37000 iterations: 0.4204081632653061\n",
            "Loss for 38000 iterations: 0.4204081632653061\n",
            "Loss for 39000 iterations: 0.4204081632653061\n",
            "Loss for 40000 iterations: 0.4204081632653061\n",
            "Loss for 41000 iterations: 0.4204081632653061\n",
            "Loss for 42000 iterations: 0.4204081632653061\n",
            "Loss for 43000 iterations: 0.4204081632653061\n",
            "Loss for 44000 iterations: 0.4204081632653061\n",
            "Loss for 45000 iterations: 0.4204081632653061\n",
            "Loss for 46000 iterations: 0.4204081632653061\n",
            "Loss for 47000 iterations: 0.4204081632653061\n",
            "Loss for 48000 iterations: 0.4204081632653061\n",
            "Loss for 49000 iterations: 0.4204081632653061\n",
            "Loss for 50000 iterations: 0.4204081632653061\n",
            "Loss for 51000 iterations: 0.4204081632653061\n",
            "Loss for 52000 iterations: 0.4204081632653061\n",
            "Loss for 53000 iterations: 0.4204081632653061\n",
            "Loss for 54000 iterations: 0.4204081632653061\n",
            "Loss for 55000 iterations: 0.4204081632653061\n",
            "Loss for 56000 iterations: 0.4204081632653061\n",
            "Loss for 57000 iterations: 0.4204081632653061\n",
            "Loss for 58000 iterations: 0.4204081632653061\n",
            "Loss for 59000 iterations: 0.4204081632653061\n",
            "Loss for 60000 iterations: 0.4204081632653061\n",
            "Loss for 61000 iterations: 0.4204081632653061\n",
            "Loss for 62000 iterations: 0.4204081632653061\n",
            "Loss for 63000 iterations: 0.4204081632653061\n",
            "Loss for 64000 iterations: 0.4204081632653061\n",
            "Loss for 65000 iterations: 0.4204081632653061\n",
            "Loss for 66000 iterations: 0.4204081632653061\n",
            "Loss for 67000 iterations: 0.4204081632653061\n",
            "Loss for 68000 iterations: 0.4204081632653061\n",
            "Loss for 69000 iterations: 0.4204081632653061\n",
            "Loss for 70000 iterations: 0.4204081632653061\n",
            "Loss for 71000 iterations: 0.4204081632653061\n",
            "Loss for 72000 iterations: 0.4204081632653061\n",
            "Loss for 73000 iterations: 0.4204081632653061\n",
            "Loss for 74000 iterations: 0.4204081632653061\n",
            "Loss for 75000 iterations: 0.4204081632653061\n",
            "Loss for 76000 iterations: 0.4204081632653061\n",
            "Loss for 77000 iterations: 0.4204081632653061\n",
            "Loss for 78000 iterations: 0.4204081632653061\n",
            "Loss for 79000 iterations: 0.4204081632653061\n",
            "Loss for 80000 iterations: 0.4204081632653061\n",
            "Loss for 81000 iterations: 0.4204081632653061\n",
            "Loss for 82000 iterations: 0.4204081632653061\n",
            "Loss for 83000 iterations: 0.4204081632653061\n",
            "Loss for 84000 iterations: 0.4204081632653061\n",
            "Loss for 85000 iterations: 0.4204081632653061\n",
            "Loss for 86000 iterations: 0.4204081632653061\n",
            "Loss for 87000 iterations: 0.4204081632653061\n",
            "Loss for 88000 iterations: 0.4204081632653061\n",
            "Loss for 89000 iterations: 0.4204081632653061\n",
            "Loss for 90000 iterations: 0.4204081632653061\n",
            "Loss for 91000 iterations: 0.4204081632653061\n",
            "Loss for 92000 iterations: 0.4204081632653061\n",
            "Loss for 93000 iterations: 0.4204081632653061\n",
            "Loss for 94000 iterations: 0.4204081632653061\n",
            "Loss for 95000 iterations: 0.4204081632653061\n",
            "Loss for 96000 iterations: 0.4204081632653061\n",
            "Loss for 97000 iterations: 0.4204081632653061\n",
            "Loss for 98000 iterations: 0.4204081632653061\n",
            "Loss for 99000 iterations: 0.4204081632653061\n",
            "Loss for 100000 iterations: 0.4204081632653061\n",
            "Loss for 101000 iterations: 0.4204081632653061\n",
            "Loss for 102000 iterations: 0.4204081632653061\n",
            "Loss for 103000 iterations: 0.4204081632653061\n",
            "Loss for 104000 iterations: 0.4204081632653061\n",
            "Loss for 105000 iterations: 0.4204081632653061\n",
            "Loss for 106000 iterations: 0.4204081632653061\n",
            "Loss for 107000 iterations: 0.4204081632653061\n",
            "Loss for 108000 iterations: 0.4204081632653061\n",
            "Loss for 109000 iterations: 0.4204081632653061\n",
            "Loss for 110000 iterations: 0.4204081632653061\n",
            "Loss for 111000 iterations: 0.4204081632653061\n",
            "Loss for 112000 iterations: 0.4204081632653061\n",
            "Loss for 113000 iterations: 0.4204081632653061\n",
            "Loss for 114000 iterations: 0.4204081632653061\n",
            "Loss for 115000 iterations: 0.4204081632653061\n",
            "Loss for 116000 iterations: 0.4204081632653061\n",
            "Loss for 117000 iterations: 0.4204081632653061\n",
            "Loss for 118000 iterations: 0.4204081632653061\n",
            "Loss for 119000 iterations: 0.4204081632653061\n",
            "Loss for 120000 iterations: 0.4204081632653061\n",
            "Loss for 121000 iterations: 0.4204081632653061\n",
            "Loss for 122000 iterations: 0.4204081632653061\n",
            "Loss for 123000 iterations: 0.4204081632653061\n",
            "Loss for 124000 iterations: 0.4204081632653061\n",
            "Loss for 125000 iterations: 0.4204081632653061\n",
            "Loss for 126000 iterations: 0.4204081632653061\n",
            "Loss for 127000 iterations: 0.4204081632653061\n",
            "Loss for 128000 iterations: 0.4204081632653061\n",
            "Loss for 129000 iterations: 0.4204081632653061\n",
            "Loss for 130000 iterations: 0.4204081632653061\n",
            "Loss for 131000 iterations: 0.4204081632653061\n",
            "Loss for 132000 iterations: 0.4204081632653061\n",
            "Loss for 133000 iterations: 0.4204081632653061\n",
            "Loss for 134000 iterations: 0.4204081632653061\n",
            "Loss for 135000 iterations: 0.4204081632653061\n",
            "Loss for 136000 iterations: 0.4204081632653061\n",
            "Loss for 137000 iterations: 0.4204081632653061\n",
            "Loss for 138000 iterations: 0.4204081632653061\n",
            "Loss for 139000 iterations: 0.4204081632653061\n",
            "Loss for 140000 iterations: 0.4204081632653061\n",
            "Loss for 141000 iterations: 0.4204081632653061\n",
            "Loss for 142000 iterations: 0.4204081632653061\n",
            "Loss for 143000 iterations: 0.4204081632653061\n",
            "Loss for 144000 iterations: 0.4204081632653061\n",
            "Loss for 145000 iterations: 0.4204081632653061\n",
            "Loss for 146000 iterations: 0.4204081632653061\n",
            "Loss for 147000 iterations: 0.4204081632653061\n",
            "Loss for 148000 iterations: 0.4204081632653061\n",
            "Loss for 149000 iterations: 0.4204081632653061\n",
            "Loss for 150000 iterations: 0.4204081632653061\n",
            "Loss for 151000 iterations: 0.4204081632653061\n",
            "Loss for 152000 iterations: 0.4204081632653061\n",
            "Loss for 153000 iterations: 0.4204081632653061\n",
            "Loss for 154000 iterations: 0.4204081632653061\n",
            "Loss for 155000 iterations: 0.4204081632653061\n",
            "Loss for 156000 iterations: 0.4204081632653061\n",
            "Loss for 157000 iterations: 0.4204081632653061\n",
            "Loss for 158000 iterations: 0.4204081632653061\n",
            "Loss for 159000 iterations: 0.4204081632653061\n",
            "Loss for 160000 iterations: 0.4204081632653061\n",
            "Loss for 161000 iterations: 0.4204081632653061\n",
            "Loss for 162000 iterations: 0.4204081632653061\n",
            "Loss for 163000 iterations: 0.4204081632653061\n",
            "Loss for 164000 iterations: 0.4204081632653061\n",
            "Loss for 165000 iterations: 0.4204081632653061\n",
            "Loss for 166000 iterations: 0.4204081632653061\n",
            "Loss for 167000 iterations: 0.4204081632653061\n",
            "Loss for 168000 iterations: 0.4204081632653061\n",
            "Loss for 169000 iterations: 0.4204081632653061\n",
            "Loss for 170000 iterations: 0.4204081632653061\n",
            "Loss for 171000 iterations: 0.4204081632653061\n",
            "Loss for 172000 iterations: 0.4204081632653061\n",
            "Loss for 173000 iterations: 0.4204081632653061\n",
            "Loss for 174000 iterations: 0.4204081632653061\n",
            "Loss for 175000 iterations: 0.4204081632653061\n",
            "Loss for 176000 iterations: 0.4204081632653061\n",
            "Loss for 177000 iterations: 0.4204081632653061\n",
            "Loss for 178000 iterations: 0.4204081632653061\n",
            "Loss for 179000 iterations: 0.4204081632653061\n",
            "Loss for 180000 iterations: 0.4204081632653061\n",
            "Loss for 181000 iterations: 0.4204081632653061\n",
            "Loss for 182000 iterations: 0.4204081632653061\n",
            "Loss for 183000 iterations: 0.4204081632653061\n",
            "Loss for 184000 iterations: 0.4204081632653061\n",
            "Loss for 185000 iterations: 0.4204081632653061\n",
            "Loss for 186000 iterations: 0.4204081632653061\n",
            "Loss for 187000 iterations: 0.4204081632653061\n",
            "Loss for 188000 iterations: 0.4204081632653061\n",
            "Loss for 189000 iterations: 0.4204081632653061\n",
            "Loss for 190000 iterations: 0.4204081632653061\n",
            "Loss for 191000 iterations: 0.4204081632653061\n",
            "Loss for 192000 iterations: 0.4204081632653061\n",
            "Loss for 193000 iterations: 0.4204081632653061\n",
            "Loss for 194000 iterations: 0.4204081632653061\n",
            "Loss for 195000 iterations: 0.4204081632653061\n",
            "Loss for 196000 iterations: 0.4204081632653061\n",
            "Loss for 197000 iterations: 0.4204081632653061\n",
            "Loss for 198000 iterations: 0.4204081632653061\n",
            "Loss for 199000 iterations: 0.4204081632653061\n",
            "Loss for 200000 iterations: 0.4204081632653061\n",
            "Loss for 201000 iterations: 0.4204081632653061\n",
            "Loss for 202000 iterations: 0.4204081632653061\n",
            "Loss for 203000 iterations: 0.4204081632653061\n",
            "Loss for 204000 iterations: 0.4204081632653061\n",
            "Loss for 205000 iterations: 0.4204081632653061\n",
            "Loss for 206000 iterations: 0.4204081632653061\n",
            "Loss for 207000 iterations: 0.4204081632653061\n",
            "Loss for 208000 iterations: 0.4204081632653061\n",
            "Loss for 209000 iterations: 0.4204081632653061\n",
            "Loss for 210000 iterations: 0.4204081632653061\n",
            "Loss for 211000 iterations: 0.4204081632653061\n",
            "Loss for 212000 iterations: 0.4204081632653061\n",
            "Loss for 213000 iterations: 0.4204081632653061\n",
            "Loss for 214000 iterations: 0.4204081632653061\n",
            "Loss for 215000 iterations: 0.4204081632653061\n",
            "Loss for 216000 iterations: 0.4204081632653061\n",
            "Loss for 217000 iterations: 0.4204081632653061\n",
            "Loss for 218000 iterations: 0.4204081632653061\n",
            "Loss for 219000 iterations: 0.4204081632653061\n",
            "Loss for 220000 iterations: 0.4204081632653061\n",
            "Loss for 221000 iterations: 0.4204081632653061\n",
            "Loss for 222000 iterations: 0.4204081632653061\n",
            "Loss for 223000 iterations: 0.4204081632653061\n",
            "Loss for 224000 iterations: 0.4204081632653061\n",
            "Loss for 225000 iterations: 0.4204081632653061\n",
            "Loss for 226000 iterations: 0.4204081632653061\n",
            "Loss for 227000 iterations: 0.4204081632653061\n",
            "Loss for 228000 iterations: 0.4204081632653061\n",
            "Loss for 229000 iterations: 0.4204081632653061\n",
            "Loss for 230000 iterations: 0.4204081632653061\n",
            "Loss for 231000 iterations: 0.4204081632653061\n",
            "Loss for 232000 iterations: 0.4204081632653061\n",
            "Loss for 233000 iterations: 0.4204081632653061\n",
            "Loss for 234000 iterations: 0.4204081632653061\n",
            "Loss for 235000 iterations: 0.4204081632653061\n",
            "Loss for 236000 iterations: 0.4204081632653061\n",
            "Loss for 237000 iterations: 0.4204081632653061\n",
            "Loss for 238000 iterations: 0.4204081632653061\n",
            "Loss for 239000 iterations: 0.4204081632653061\n",
            "Loss for 240000 iterations: 0.4204081632653061\n",
            "Loss for 241000 iterations: 0.4204081632653061\n",
            "Loss for 242000 iterations: 0.4204081632653061\n",
            "Loss for 243000 iterations: 0.4204081632653061\n",
            "Loss for 244000 iterations: 0.4204081632653061\n",
            "Loss for 245000 iterations: 0.4204081632653061\n",
            "Loss for 246000 iterations: 0.4204081632653061\n",
            "Loss for 247000 iterations: 0.4204081632653061\n",
            "Loss for 248000 iterations: 0.4204081632653061\n",
            "Loss for 249000 iterations: 0.4204081632653061\n",
            "Loss for 250000 iterations: 0.4204081632653061\n",
            "[[ 1.19828765]\n",
            " [-1.49511849]\n",
            " [-2.05539845]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgLDOdzr0eSq",
        "outputId": "e5433d82-66b2-4214-f013-d43304072419"
      },
      "source": [
        "y_pred = np.dot(X_test,opt_theta)\n",
        "loss_y = y_pred - y_test\n",
        "\n",
        "test_size = y_pred.shape[0]\n",
        "total_loss_y = np.dot(np.ones((1,test_size)),np.square(loss_y))[0,0] / test_size\n",
        "\n",
        "# print(total_loss_y)\n",
        "\n",
        "y_pred_thresh = y_pred>=0.5\n",
        "\n",
        "tp = np.sum((y_pred_thresh+y_test)==2 , axis=0)[0]\n",
        "tn = np.sum(y_pred_thresh==y_test , axis=0)[0] - tp\n",
        "fp = np.sum(y_pred_thresh , axis=0)[0]-tp\n",
        "fn = test_size-tp-tn-fp\n",
        "\n",
        "\n",
        "print('tp: {} , tn: {} , fp: {} , fn: {}'.format(tp,tn,fp,fn))\n",
        "\n",
        "acc = (tp+tn)/test_size\n",
        "prec = (tp)/(tp+fp)\n",
        "recl = (tp)/(tp+fn)\n",
        "f1 = 2*prec*recl/(prec+recl)\n",
        "\n",
        "print('Accuracy: {}'.format( acc  ))\n",
        "print('Precision: {}'.format( prec  ))\n",
        "print('Recall: {}'.format( recl  ))\n",
        "print('F1 score: {}'.format( f1  ))\n",
        "\n",
        "\n"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tp: 0 , tn: 116 , fp: 0 , fn: 94\nAccuracy: 0.5523809523809524\nPrecision: nan\nRecall: 0.0\nF1 score: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwYJbW3h0ncU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}