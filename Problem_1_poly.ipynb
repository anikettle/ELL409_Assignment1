{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ret(degree=1,split_percent=70):\n",
    "    \n",
    "    pwd = os.getcwd()+'\\\\health_data.csv'\n",
    "    dataset = pd.read_csv(pwd)             \n",
    "    dataset = dataset.sample(frac = 1)\n",
    "    X = dataset.iloc[:,:-1].values\n",
    "    y = dataset.iloc[:,-1:].values\n",
    "    datasize = X.shape[0]\n",
    "    split_point = split_percent//10\n",
    "    X_train = X[:(datasize*split_point)//10,:]\n",
    "    y_train = y[:(datasize*split_point)//10,:]\n",
    "    X_test = X[(datasize*split_point)//10:,:]\n",
    "    y_test = y[(datasize*split_point)//10:,:]\n",
    "\n",
    "\n",
    "\n",
    "    X_train_mean = np.sum(X_train,axis=0).reshape(1,-1)/X_train.shape[0]\n",
    "    X_train_var = np.sqrt(np.sum(np.square(X_train-X_train_mean),axis=0).reshape(1,-1)) / X_train.shape[0]\n",
    "    X_train = (X_train-X_train_mean) / X_train_var\n",
    "    X_test = (X_test-X_train_mean) / X_train_var\n",
    "    \n",
    "\n",
    "    X_train_ret,X_test_ret = X_train,X_test\n",
    "    X_train_temp = X_train\n",
    "    X_test_temp = X_test\n",
    "\n",
    "    for i in range(2,degree+1):\n",
    "        X_train_temp = np.multiply(X_train_temp,X_train)\n",
    "        X_test_temp = np.multiply(X_test_temp,X_test)\n",
    "        X_train_temp_var = np.sqrt(np.sum(np.square(X_train_temp),axis=0).reshape(1,-1)) / X_train.shape[0]\n",
    "\n",
    "        X_train_temp = X_train_temp / (X_train_temp_var)\n",
    "        X_test_temp = X_test_temp / (X_train_temp_var)\n",
    "\n",
    "        X_train_ret = np.append(X_train_ret,X_train_temp,axis=1)\n",
    "        X_test_ret = np.append(X_test_ret,X_test_temp,axis=1)\n",
    "\n",
    "    X_train_ret = np.append(np.ones((X_train.shape[0],1)),X_train_ret,axis=1)\n",
    "    X_test_ret = np.append(np.ones((X_test.shape[0],1)),X_test_ret,axis=1)\n",
    "\n",
    "    return X_train_ret,y_train,X_test_ret,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ret2(split_percent=70):\n",
    "    \n",
    "\n",
    "    dataset = pd.read_excel('E:\\ELL_Project\\problem2\\weather_data.xlsx')                     \n",
    "    dataset = dataset.sample(frac = 1)\n",
    "    X = dataset.iloc[:,:-1].values\n",
    "    y = dataset.iloc[:,-1:].values\n",
    "    datasize = X.shape[0]\n",
    "    split_point = split_percent//10\n",
    "    X_train = X[:(datasize*split_point)//10,:]\n",
    "    y_train = y[:(datasize*split_point)//10,:]\n",
    "    X_test = X[(datasize*split_point)//10:,:]\n",
    "    y_test = y[(datasize*split_point)//10:,:]\n",
    "    return (X_train,X_test,y_train,y_test)\n",
    "\n",
    "def datapol2(X_train,X_test,y_train,y_test,degree=1):\n",
    "\n",
    "    X_train_mean = np.sum(X_train,axis=0).reshape(1,-1)/X_train.shape[0]\n",
    "    X_train_var = np.sqrt(np.sum(np.square(X_train-X_train_mean),axis=0).reshape(1,-1)) / X_train.shape[0]\n",
    "    X_train = (X_train-X_train_mean) / X_train_var\n",
    "    X_test = (X_test-X_train_mean) / X_train_var\n",
    "    \n",
    "\n",
    "    X_train_ret,X_test_ret = X_train,X_test\n",
    "    X_train_temp = X_train\n",
    "    X_test_temp = X_test\n",
    "\n",
    "    for i in range(2,degree+1):\n",
    "        X_train_temp = np.multiply(X_train_temp,X_train)\n",
    "        X_test_temp = np.multiply(X_test_temp,X_test)\n",
    "        X_train_temp_var = np.sqrt(np.sum(np.square(X_train_temp),axis=0).reshape(1,-1)) / X_train.shape[0]\n",
    "\n",
    "        X_train_temp = X_train_temp / (X_train_temp_var)\n",
    "        X_test_temp = X_test_temp / (X_train_temp_var)\n",
    "\n",
    "        X_train_ret = np.append(X_train_ret,X_train_temp,axis=1)\n",
    "        X_test_ret = np.append(X_test_ret,X_test_temp,axis=1)\n",
    "\n",
    "    X_train_ret = np.append(np.ones((X_train.shape[0],1)),X_train_ret,axis=1)\n",
    "    X_test_ret = np.append(np.ones((X_test.shape[0],1)),X_test_ret,axis=1)\n",
    "\n",
    "    return X_train_ret,y_train,X_test_ret,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradDesc(X,y,theta,hypothesis,loss_function,regularizer,alpha,lambda1,lambda2):\n",
    "    h,loss,gradient = loss_function(X,theta,y,hypothesis)\n",
    "    if(regularizer==elastic_net_reg):\n",
    "        reg_loss,reg_grad = regularizer(lambda1,lambda2,theta)\n",
    "    else:\n",
    "        reg_loss,reg_grad = regularizer(lambda1,theta)\n",
    "    # loss += reg_loss\n",
    "    gradient += reg_grad\n",
    "\n",
    "    return(loss,gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(X,theta,y,hypothesis):\n",
    "    siz = y.shape[0]\n",
    "    h = hypothesis(X,theta)\n",
    "    diff = h-y\n",
    "    mse = ( np.sum(np.square(diff),axis=0))[0]\n",
    "    gradient = np.dot(np.transpose(X),diff) / siz\n",
    "    return(h,mse,gradient)\n",
    "\n",
    "def mae_loss(X,theta,y,hypothesis):\n",
    "    siz = y.shape[0]\n",
    "    num_feat = y.shape[1]\n",
    "    h = hypothesis(X,theta)\n",
    "    diff = h-y\n",
    "    diff_sign = np.ones((siz,1))\n",
    "    diff_sign[diff[:,0]<0] = -1\n",
    "    mae = np.sum(np.abs(h),axis=0)[0] / siz\n",
    "    gradient = np.sum(diff_sign*X,axis=0).reshape(-1,1)\n",
    "    return(h,mae,gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_hyp(X,theta):\n",
    "    # print(np.dot(X,theta).shape)\n",
    "    return (np.dot(X,theta))\n",
    "def log_hyp(X,theta):\n",
    "    a = lin_hyp(X,theta)\n",
    "    return ( 1/(1+np.exp(-a)) )\n",
    "def null_regularizer(alpha,theta):\n",
    "    a = np.zeros(theta.shape)\n",
    "    return(0,a)\n",
    "def l1_reg(alpha,theta):\n",
    "    reg_loss = alpha*theta\n",
    "    reg_grad = alpha\n",
    "    return(reg_loss,reg_grad)\n",
    "def l2_reg(alpha,theta):\n",
    "    reg_loss = alpha * np.square(theta)\n",
    "    reg_grad = 2 * alpha * theta\n",
    "    return(reg_loss,reg_grad)\n",
    "def elastic_net_reg(lambda1,lambda2,theta):\n",
    "    a1,a2 = l1_reg(lambda1,theta)\n",
    "    b1,b2 = l2_reg(lambda2,theta)\n",
    "    return( a1+b1 , a2+b2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linReg(X,y,iter=1000,alpha=0.01,batchSize=32,hypothesis = lin_hyp,loss_function=mse_loss,regularizer=l2_reg,lambda1 = 0.01,lambd2_elasnet = 0.01):\n",
    "    \n",
    "    theta = np.random.random((X.shape[1],1))\n",
    "    ret_thet = []\n",
    "    # print(theta)\n",
    "    # print(theta)\n",
    "    datasize = X.shape[0]\n",
    "    loss_list = []\n",
    "    loss_epoch,_ = gradDesc(X,y,theta,hypothesis,loss_function,regularizer,alpha,lambda1, lambd2_elasnet)\n",
    "    for i in range(iter):\n",
    "        if((i+1)%1000==0):\n",
    "            loss_list.append(loss_epoch)\n",
    "            print('Loss for {} iterations: {}'.format(i+1,loss_epoch))\n",
    "        fro = 0\n",
    "        loss_epoch = 0\n",
    "        while(True):\n",
    "            to = min(fro+batchSize,datasize)\n",
    "            l,theta_grad = gradDesc(X[fro:to,:],y[fro:to,:],theta,hypothesis,loss_function,regularizer,alpha,lambda1, lambd2_elasnet)\n",
    "            # print(theta_grad)\n",
    "            loss_epoch += l\n",
    "            \n",
    "            # print(theta_grad)\n",
    "            theta -= (alpha*theta_grad)\n",
    "            ret_thet.append(theta)\n",
    "            ll,_ = gradDesc(X,y,theta,hypothesis,loss_function,regularizer,alpha,lambda1, lambd2_elasnet)\n",
    "            loss_list.append(ll)\n",
    "            fro = to\n",
    "\n",
    "            if(to>=datasize):\n",
    "                break\n",
    "        \n",
    "    # print(ret_thet)\n",
    "    return (theta,loss_epoch,loss_list,ret_thet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metrics(y_pred,y_test):\n",
    "    loss_y = y_pred - y_test\n",
    "\n",
    "    test_size = y_pred.shape[0]\n",
    "    total_loss_y = np.dot(np.ones((1,test_size)),np.square(loss_y))[0,0] / test_size\n",
    "\n",
    "    # print(total_loss_y)\n",
    "\n",
    "    y_pred_thresh = y_pred>=0.5\n",
    "\n",
    "    tp = np.sum((y_pred_thresh+y_test)==2 , axis=0)[0]\n",
    "    tn = np.sum(y_pred_thresh==y_test , axis=0)[0] - tp\n",
    "    fp = np.sum(y_pred_thresh , axis=0)[0]-tp\n",
    "    fn = test_size-tp-tn-fp\n",
    "\n",
    "\n",
    "    print('tp: {} , tn: {} , fp: {} , fn: {}'.format(tp,tn,fp,fn))\n",
    "\n",
    "    acc = (tp+tn)/test_size\n",
    "    prec = (tp)/(tp+fp)\n",
    "    recl = (tp)/(tp+fn)\n",
    "    f1 = 2*prec*recl/(prec+recl)\n",
    "\n",
    "    print('Accuracy: {}'.format( acc  ))\n",
    "    print('Precision: {}'.format( prec  ))\n",
    "    print('Recall: {}'.format( recl  ))\n",
    "    print('F1 score: {}'.format( f1  ))\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss for 1000 iterations: 384.4201381865984\n",
      "Loss for 2000 iterations: 225.72543800384946\n",
      "Loss for 3000 iterations: 149.07459453426546\n",
      "Loss for 4000 iterations: 106.24531291185792\n",
      "Loss for 5000 iterations: 81.29997933741092\n",
      "Loss for 6000 iterations: 66.41967976475692\n",
      "Loss for 7000 iterations: 57.422531419158666\n",
      "Loss for 8000 iterations: 51.9443929374737\n",
      "Loss for 9000 iterations: 48.600322176286156\n",
      "Loss for 10000 iterations: 46.56107675945438\n",
      "Loss for 11000 iterations: 45.32357762629897\n",
      "Loss for 12000 iterations: 44.58020139314743\n",
      "Loss for 13000 iterations: 44.14192885862866\n",
      "Loss for 14000 iterations: 43.89225496874996\n",
      "Loss for 15000 iterations: 43.75921393565872\n",
      "Loss for 16000 iterations: 43.698276465866506\n",
      "Loss for 17000 iterations: 43.6818476653655\n",
      "Loss for 18000 iterations: 43.69280033671073\n",
      "Loss for 19000 iterations: 43.720485840332834\n",
      "Loss for 20000 iterations: 43.7582701059238\n",
      "tp: 75 , tn: 108 , fp: 12 , fn: 15\n",
      "Accuracy: 0.8714285714285714\n",
      "Precision: 0.8620689655172413\n",
      "Recall: 0.8333333333333334\n",
      "F1 score: 0.847457627118644\n"
     ]
    }
   ],
   "source": [
    "X_train,y_train,X_test,y_test = data_ret(degree=5,split_percent=70)\n",
    "train_datasize = X_train.shape[0]\n",
    "opt_theta, train_loss , loss_list , ret_thet = linReg(X_train,y_train,iter=20000,alpha=0.0001,batchSize=10000,hypothesis = lin_hyp,loss_function=mse_loss,regularizer=l2_reg,lambda1=0.1,lambd2_elasnet = 0.001)\n",
    "# y_pred_train = lin_hyp(X_train,opt_theta)\n",
    "y_pred_test = lin_hyp(X_test,opt_theta)\n",
    "# mse_tr,_ = accuracy(y_pred_train,y_train)\n",
    "mse_te = accuracy_metrics(y_pred_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}