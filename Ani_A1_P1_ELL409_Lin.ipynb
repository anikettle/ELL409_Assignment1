{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ani_A1_P1_ELL409_Lin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPZozwobkXw5"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6HP6dinkbGy"
      },
      "source": [
        "def data_ret():\n",
        "    dataset = pd.read_csv('E:\\ELL_project\\problem1\\health_data.csv')\n",
        "    dataset = dataset.sample(frac = 1)\n",
        "    X = dataset.iloc[:,:-1].values\n",
        "    y = dataset.iloc[:,-1:].values\n",
        "    datasize = X.shape[0]\n",
        "    X_train = X[:(datasize*7)//10,:]\n",
        "    y_train = y[:(datasize*7)//10,:]\n",
        "    X_test = X[(datasize*7)//10:,:]\n",
        "    y_test = y[(datasize*7)//10:,:]\n",
        "\n",
        "    X_train_mean = np.sum(X_train,axis=0).reshape(1,-1)/X_train.shape[0]\n",
        "    X_train_var = np.sqrt(np.sum(np.square(X_train-X_train_mean),axis=0).reshape(1,-1)) / X_train.shape[0]\n",
        "    X_train = (X_train-X_train_mean) / X_train_var\n",
        "    X_train = np.append(np.ones((X_train.shape[0],1)),X_train,axis=1)\n",
        "    X_test = (X_test-X_train_mean) / X_train_var\n",
        "    X_test = np.append(np.ones((X_test.shape[0],1)),X_test,axis=1)\n",
        "    return X_train,y_train,X_test,y_test"
      ],
      "execution_count": 309,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzIp7YTSnDsD"
      },
      "source": [
        "def gradDesc(X,y,theta,hypothesis,loss_function,regularizer,alpha,lambda1,lambda2):\n",
        "    h,loss,gradient = loss_function(X,theta,y,hypothesis)\n",
        "    if(regularizer==elastic_net_reg):\n",
        "        reg_loss,reg_grad = regularizer(lambda1,lambda2,theta)\n",
        "    else:\n",
        "        reg_loss,reg_grad = regularizer(lambda1,theta)\n",
        "    # loss += reg_loss\n",
        "    gradient += reg_grad\n",
        "\n",
        "    return(loss,gradient)"
      ],
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mse_loss(X,theta,y,hypothesis):\n",
        "    siz = y.shape[0]\n",
        "    h = hypothesis(X,theta)\n",
        "    diff = h-y\n",
        "    mse = ( np.sum(np.square(diff),axis=0))[0]\n",
        "    gradient = np.dot(np.transpose(X),diff) / siz\n",
        "    return(h,mse,gradient)\n",
        "\n",
        "def mae_loss(X,theta,y,hypothesis):\n",
        "    siz = y.shape[0]\n",
        "    num_feat = y.shape[1]\n",
        "    h = hypothesis(X,theta)\n",
        "    diff = h-y\n",
        "    diff_sign = np.ones((siz,1))\n",
        "    diff_sign[diff[:,0]<0] = -1\n",
        "    mae = np.sum(np.abs(h),axis=0)[0] / siz\n",
        "    gradient = np.sum(diff_sign*X,axis=0).reshape(-1,1)\n",
        "    return(h,mae,gradient)\n",
        "\n",
        "def ce_loss(X,theta,y,hypothesis):\n",
        "    siz = y.shape[0]\n",
        "    h = hypothesis(X,theta)\n",
        "    h_comp = 1-h\n",
        "    diff = h - y\n",
        "    ce = np.sum(-y*np.log(h)-(1-y)*np.log(1-h),axis=0) [0]\n",
        "    gradient = np.dot(np.transpose(X),diff).reshape(-1,1) / siz\n",
        "    return(h,ce,gradient)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lin_hyp(X,theta):\n",
        "    # print(np.dot(X,theta).shape)\n",
        "    return (np.dot(X,theta))\n",
        "def log_hyp(X,theta):\n",
        "    a = lin_hyp(X,theta)\n",
        "    return ( 1/(1+np.exp(-a)) )\n",
        "def null_regularizer(alpha,theta):\n",
        "    a = np.zeros(theta.shape)\n",
        "    return(0,a)\n",
        "def l1_reg(alpha,theta):\n",
        "    reg_loss = alpha*theta\n",
        "    reg_grad = alpha\n",
        "    return(reg_loss,reg_grad)\n",
        "def l2_reg(alpha,theta):\n",
        "    reg_loss = alpha * np.square(theta)\n",
        "    reg_grad = 2 * alpha * theta\n",
        "    return(reg_loss,reg_grad)\n",
        "def elastic_net_reg(lambda1,lambda2,theta):\n",
        "    a1,a2 = l1_reg(lambda1,theta)\n",
        "    b1,b2 = l2_reg(lambda2,theta)\n",
        "    return( a1+b1 , a2+b2 )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV5jirTckbxC"
      },
      "source": [
        "def linReg(X,y,iter=1000,alpha=0.01,batchSize=32,hypothesis = lin_hyp,loss_function=mse_loss,regularizer=l2_reg,lambda1 = 0.01,lambd2_elasnet = 0.01):\n",
        "    theta = np.random.random((X.shape[1],1))\n",
        "    # print(theta)\n",
        "    # print(theta)\n",
        "    datasize = X.shape[0]\n",
        "    loss_epoch = 0\n",
        "    for i in range(iter):\n",
        "        if((i+1)%100==0 and i>0):\n",
        "            print('Loss for {} iterations: {}'.format(i+1,loss_epoch))\n",
        "        fro = 0\n",
        "        loss_epoch = 0\n",
        "        while(True):\n",
        "            to = min(fro+batchSize,datasize)\n",
        "            l,theta_grad = gradDesc(X[fro:to,:],y[fro:to,:],theta,hypothesis,loss_function,regularizer,alpha,lambda1, lambd2_elasnet)\n",
        "            # print(theta_grad)\n",
        "            loss_epoch += l\n",
        "            # print(theta_grad)\n",
        "            theta -= (alpha*theta_grad)\n",
        "            fro = to\n",
        "\n",
        "            if(to>=datasize):\n",
        "                break\n",
        "    \n",
        "    return (theta,loss_epoch)\n"
      ],
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm8k4nHDlNIj",
        "outputId": "a1327632-f176-436b-a9d4-b004c526f381",
        "tags": []
      },
      "source": [
        "X_train,y_train,X_test,y_test = data_ret()\n",
        "train_datasize = X_train.shape[0]\n",
        "opt_theta, train_loss = linReg(X_train,y_train,iter=1000,alpha=0.001,batchSize=5,hypothesis = lin_hyp,loss_function=mse_loss,regularizer=l1_reg,lambda1=0.01,lambd2_elasnet = 0.001)\n",
        "print(opt_theta)"
      ],
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss for 100 iterations: 52.49571111884227\n",
            "Loss for 200 iterations: 52.49575782710468\n",
            "Loss for 300 iterations: 52.495757829391806\n",
            "Loss for 400 iterations: 52.495757829391934\n",
            "Loss for 500 iterations: 52.495757829391934\n",
            "Loss for 600 iterations: 52.495757829391934\n",
            "Loss for 700 iterations: 52.495757829391934\n",
            "Loss for 800 iterations: 52.495757829391934\n",
            "Loss for 900 iterations: 52.495757829391934\n",
            "Loss for 1000 iterations: 52.495757829391934\n",
            "[[ 4.10957068e-01]\n",
            " [ 1.53491848e-02]\n",
            " [ 7.15529401e-03]\n",
            " [-3.36786273e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfR0iO_y78aw",
        "outputId": "c9482015-ded4-42dc-88b7-b1c6cfdd4d2e"
      },
      "source": [
        "y_pred = np.dot(X_test,opt_theta)\n",
        "loss_y = y_pred - y_test\n",
        "\n",
        "test_size = y_pred.shape[0]\n",
        "total_loss_y = np.dot(np.ones((1,test_size)),np.square(loss_y))[0,0] / test_size\n",
        "\n",
        "y_pred_thresh = y_pred>=0.5\n",
        "\n",
        "tp = np.sum((y_pred_thresh+y_test)==2 , axis=0)[0]\n",
        "tn = np.sum(y_pred_thresh==y_test , axis=0)[0] - tp\n",
        "fp = np.sum(y_pred_thresh , axis=0)[0]-tp\n",
        "fn = test_size-tp-tn-fp\n",
        "\n",
        "print('tp: {} , tn: {} , fp: {} , fn: {}'.format(tp,tn,fp,fn))\n",
        "\n",
        "acc = (tp+tn)/test_size\n",
        "prec = (tp)/(tp+fp)\n",
        "recl = (tp)/(tp+fn)\n",
        "f1 = 2*prec*recl/(prec+recl)\n",
        "print('Accuracy: {}'.format( acc  ))\n",
        "print('Precision: {}'.format( prec  ))\n",
        "print('Recall: {}'.format( recl  ))\n",
        "print('F1 score: {}'.format( f1  ))"
      ],
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tp: 0 , tn: 134 , fp: 0 , fn: 76\nAccuracy: 0.638095238095238\nPrecision: nan\nRecall: 0.0\nF1 score: nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ronMH15GLU4"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "\n",
        "# x = np.array([1,2,3,4,5])\n",
        "# y = np.array([2,1,3,6,7])\n",
        "\n",
        "# cluster = np.array([1,1,1,2,2]) \n",
        "\n",
        "# fig, ax = plt.subplots()\n",
        "\n",
        "# ax.scatter(x[cluster==1],y[cluster==1], marker='^')\n",
        "# ax.scatter(x[cluster==2],y[cluster==2], marker='s')\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": 214,
      "outputs": []
    }
  ]
}