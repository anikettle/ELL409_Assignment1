{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('E:\\ELL_project\\problem1\\health_data.csv')\n",
    "dataset = dataset.sample(frac = 1)\n",
    "X = dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,-1:].values\n",
    "datasize = X.shape[0]\n",
    "X_train = X[:(datasize*7)//10,:]\n",
    "y_train = y[:(datasize*7)//10,:]\n",
    "X_test = X[(datasize*7)//10:,:]\n",
    "y_test = y[(datasize*7)//10:,:]\n",
    "\n",
    "# X_train_size = X_train.shape[0]\n",
    "# X_train = np.append(np.ones((X_train_size,1),X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradDesc(X,y,theta):\n",
    "    h  = np.dot(X,theta)\n",
    "    dsc = X.shape[0]\n",
    "    loss = h-y\n",
    "    total_loss = np.dot(np.ones((1,dsc)),np.square(loss))[0,0] /dsc\n",
    "    update = np.dot(np.transpose(X),loss)\n",
    "    update /= dsc\n",
    "    return(total_loss,update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linReg(X,y,iter=100,alpha=0.01,batchSize=32):\n",
    "    # print(X.shape)\n",
    "    theta = np.random.random((X.shape[1],1))\n",
    "    # print(theta)\n",
    "    datasize = X.shape[0]\n",
    "    loss_epoch = 0\n",
    "    for i in range(iter):\n",
    "        if((i+1)%1000==0 and i>0):\n",
    "            print('Loss for {} iterations: {}'.format(i+1,loss_epoch))\n",
    "        fro = 0\n",
    "        loss_epoch = 0\n",
    "        while(True):\n",
    "            to = min(fro+batchSize,datasize)\n",
    "            l,theta_grad = gradDesc(X[fro:to,:],y[fro:to,:],theta)\n",
    "            # print(theta_grad)\n",
    "            loss_epoch += l\n",
    "            theta -= (alpha*theta_grad)\n",
    "            fro = to\n",
    "\n",
    "            if(to>=datasize):\n",
    "                break\n",
    "    \n",
    "    return (theta,loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(X_train):\n",
    "    training_size = X_train.shape[0]\n",
    "    X_mean = np.sum(X_train,axis=0) / training_size\n",
    "    X_var = np.sqrt(np.sum((np.square(X-X_mean)),axis=0)/training_size)\n",
    "    X_train_reg = (X_train - X_mean) / X_var\n",
    "    return (X_mean,X_var,X_train_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_feat(X,degree=2):\n",
    "    num_feats = X.shape[1]\n",
    "    num_vals = X.shape[0]\n",
    "    X_cross = X\n",
    "    for i in range(num_feats):\n",
    "        for j in range(i+1,num_feats):\n",
    "            X_cross = np.append( X_cross ,  np.multiply ( X[:,i:i+1] , X[:,j:j+1] ) , axis=1 )\n",
    "    for i in range(3,degree):\n",
    "        X_power = np.power(X,i)\n",
    "        X_cross = np.append(X_cross,X_power,axis=1)\n",
    "\n",
    "    return ( X_cross )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyReg(X,y,iter=100,alpha=0.01,batchSize=32,degree=2):\n",
    "    X_mean,X_var,X_norm = feature_scaling(X)\n",
    "    X_cross = poly_feat(X_norm,degree)\n",
    "    train_size = X_cross.shape[0]\n",
    "    X_cross = np.append(np.ones((train_size,1)),X_cross,axis=1)\n",
    "    opt, tl = linReg(X_cross,y,iter=iter,alpha=alpha,batchSize=batchSize)\n",
    "    return ( X_mean, X_var, opt , tl )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loss for 1000 iterations: 0.10133692194614084\n",
      "Loss for 2000 iterations: 0.0909107299072428\n",
      "Loss for 3000 iterations: 0.08665561054595813\n",
      "Loss for 4000 iterations: 0.08470303357136973\n",
      "Loss for 5000 iterations: 0.08366616339600486\n",
      "Loss for 6000 iterations: 0.08302696978605627\n",
      "Loss for 7000 iterations: 0.08258457463792523\n",
      "Loss for 8000 iterations: 0.082255605002815\n",
      "Loss for 9000 iterations: 0.08200140064658072\n",
      "Loss for 10000 iterations: 0.08180121056327097\n",
      "Loss for 11000 iterations: 0.08164213225701573\n",
      "Loss for 12000 iterations: 0.0815151891272087\n",
      "Loss for 13000 iterations: 0.08141368950307014\n",
      "Loss for 14000 iterations: 0.08133245779805111\n",
      "Loss for 15000 iterations: 0.08126741740166235\n",
      "Loss for 16000 iterations: 0.08121532922888727\n",
      "Loss for 17000 iterations: 0.08117360900903131\n",
      "Loss for 18000 iterations: 0.08114019084224784\n",
      "Loss for 19000 iterations: 0.08111342163460941\n",
      "Loss for 20000 iterations: 0.0810919779710418\n",
      "Loss for 21000 iterations: 0.08107480009583228\n",
      "Loss for 22000 iterations: 0.08106103926309327\n",
      "Loss for 23000 iterations: 0.08105001566417591\n",
      "Loss for 24000 iterations: 0.08104118477241361\n",
      "Loss for 25000 iterations: 0.08103411040630637\n",
      "Loss for 26000 iterations: 0.08102844316324338\n",
      "Loss for 27000 iterations: 0.08102390314976558\n",
      "Loss for 28000 iterations: 0.08102026615073153\n",
      "Loss for 29000 iterations: 0.08101735255171078\n",
      "Loss for 30000 iterations: 0.0810150184660382\n",
      "Loss for 31000 iterations: 0.081013148627475\n",
      "Loss for 32000 iterations: 0.0810116506969762\n",
      "Loss for 33000 iterations: 0.08101045070211114\n",
      "Loss for 34000 iterations: 0.08100948938374072\n",
      "Loss for 35000 iterations: 0.081008719269432\n",
      "Loss for 36000 iterations: 0.0810081023290239\n",
      "Loss for 37000 iterations: 0.08100760809653065\n",
      "Loss for 38000 iterations: 0.08100721216561566\n",
      "Loss for 39000 iterations: 0.08100689498432423\n",
      "Loss for 40000 iterations: 0.0810066408895488\n",
      "Loss for 41000 iterations: 0.08100643733354096\n",
      "Loss for 42000 iterations: 0.08100627426427094\n",
      "Loss for 43000 iterations: 0.08100614362903351\n",
      "Loss for 44000 iterations: 0.0810060389767864\n",
      "Loss for 45000 iterations: 0.08100595513958306\n",
      "Loss for 46000 iterations: 0.08100588797736802\n",
      "Loss for 47000 iterations: 0.08100583417353176\n",
      "Loss for 48000 iterations: 0.08100579107112947\n",
      "Loss for 49000 iterations: 0.08100575654167542\n",
      "Loss for 50000 iterations: 0.08100572888003395\n",
      "Loss for 51000 iterations: 0.08100570672021612\n",
      "Loss for 52000 iterations: 0.08100568896792447\n",
      "Loss for 53000 iterations: 0.08100567474651427\n",
      "Loss for 54000 iterations: 0.08100566335370302\n",
      "Loss for 55000 iterations: 0.08100565422689024\n",
      "Loss for 56000 iterations: 0.08100564691537512\n",
      "Loss for 57000 iterations: 0.08100564105810003\n",
      "Loss for 58000 iterations: 0.08100563636582064\n",
      "Loss for 59000 iterations: 0.08100563260682259\n",
      "Loss for 60000 iterations: 0.08100562959547879\n",
      "Loss for 61000 iterations: 0.08100562718308288\n",
      "Loss for 62000 iterations: 0.08100562525050582\n",
      "Loss for 63000 iterations: 0.08100562370231293\n",
      "Loss for 64000 iterations: 0.08100562246205131\n",
      "Loss for 65000 iterations: 0.08100562146847427\n",
      "Loss for 66000 iterations: 0.08100562067251696\n",
      "Loss for 67000 iterations: 0.08100562003487331\n",
      "Loss for 68000 iterations: 0.08100561952405523\n",
      "Loss for 69000 iterations: 0.08100561911483746\n",
      "Loss for 70000 iterations: 0.08100561878701193\n",
      "Loss for 71000 iterations: 0.08100561852439002\n",
      "Loss for 72000 iterations: 0.08100561831400287\n",
      "Loss for 73000 iterations: 0.08100561814546112\n",
      "Loss for 74000 iterations: 0.08100561801044187\n",
      "Loss for 75000 iterations: 0.08100561790227757\n",
      "Loss for 76000 iterations: 0.08100561781562682\n",
      "Loss for 77000 iterations: 0.08100561774621069\n",
      "Loss for 78000 iterations: 0.08100561769060124\n",
      "Loss for 79000 iterations: 0.08100561764605234\n",
      "Loss for 80000 iterations: 0.08100561761036411\n",
      "Loss for 81000 iterations: 0.08100561758177416\n",
      "Loss for 82000 iterations: 0.08100561755887067\n",
      "Loss for 83000 iterations: 0.08100561754052261\n",
      "Loss for 84000 iterations: 0.08100561752582397\n",
      "Loss for 85000 iterations: 0.08100561751404879\n",
      "Loss for 86000 iterations: 0.08100561750461568\n",
      "Loss for 87000 iterations: 0.0810056174970588\n",
      "Loss for 88000 iterations: 0.08100561749100496\n",
      "Loss for 89000 iterations: 0.0810056174861552\n",
      "Loss for 90000 iterations: 0.08100561748227007\n",
      "Loss for 91000 iterations: 0.08100561747915766\n",
      "Loss for 92000 iterations: 0.0810056174766643\n",
      "Loss for 93000 iterations: 0.08100561747466689\n",
      "Loss for 94000 iterations: 0.08100561747306671\n",
      "Loss for 95000 iterations: 0.08100561747178484\n",
      "Loss for 96000 iterations: 0.08100561747075792\n",
      "Loss for 97000 iterations: 0.08100561746993526\n",
      "Loss for 98000 iterations: 0.08100561746927622\n",
      "Loss for 99000 iterations: 0.08100561746874824\n",
      "Loss for 100000 iterations: 0.08100561746832531\n",
      "Loss for 101000 iterations: 0.08100561746798648\n",
      "Loss for 102000 iterations: 0.08100561746771502\n",
      "Loss for 103000 iterations: 0.08100561746749758\n",
      "Loss for 104000 iterations: 0.08100561746732339\n",
      "Loss for 105000 iterations: 0.08100561746718384\n",
      "Loss for 106000 iterations: 0.08100561746707205\n",
      "Loss for 107000 iterations: 0.08100561746698247\n",
      "Loss for 108000 iterations: 0.08100561746691075\n",
      "Loss for 109000 iterations: 0.08100561746685327\n",
      "Loss for 110000 iterations: 0.08100561746680723\n",
      "Loss for 111000 iterations: 0.08100561746677033\n",
      "Loss for 112000 iterations: 0.0810056174667408\n",
      "Loss for 113000 iterations: 0.08100561746671713\n",
      "Loss for 114000 iterations: 0.08100561746669817\n",
      "Loss for 115000 iterations: 0.08100561746668297\n",
      "Loss for 116000 iterations: 0.08100561746667079\n",
      "Loss for 117000 iterations: 0.08100561746666105\n",
      "Loss for 118000 iterations: 0.08100561746665323\n",
      "Loss for 119000 iterations: 0.08100561746664697\n",
      "Loss for 120000 iterations: 0.08100561746664196\n",
      "Loss for 121000 iterations: 0.08100561746663794\n",
      "Loss for 122000 iterations: 0.0810056174666347\n",
      "Loss for 123000 iterations: 0.08100561746663217\n",
      "Loss for 124000 iterations: 0.08100561746663007\n",
      "Loss for 125000 iterations: 0.08100561746662843\n",
      "Loss for 126000 iterations: 0.08100561746662711\n",
      "Loss for 127000 iterations: 0.08100561746662605\n",
      "Loss for 128000 iterations: 0.08100561746662518\n",
      "Loss for 129000 iterations: 0.0810056174666245\n",
      "Loss for 130000 iterations: 0.08100561746662396\n",
      "Loss for 131000 iterations: 0.08100561746662352\n",
      "Loss for 132000 iterations: 0.08100561746662319\n",
      "Loss for 133000 iterations: 0.08100561746662291\n",
      "Loss for 134000 iterations: 0.08100561746662269\n",
      "Loss for 135000 iterations: 0.0810056174666225\n",
      "Loss for 136000 iterations: 0.08100561746662234\n",
      "Loss for 137000 iterations: 0.08100561746662224\n",
      "Loss for 138000 iterations: 0.08100561746662215\n",
      "Loss for 139000 iterations: 0.08100561746662206\n",
      "Loss for 140000 iterations: 0.081005617466622\n",
      "Loss for 141000 iterations: 0.08100561746662197\n",
      "Loss for 142000 iterations: 0.08100561746662192\n",
      "Loss for 143000 iterations: 0.0810056174666219\n",
      "Loss for 144000 iterations: 0.08100561746662187\n",
      "Loss for 145000 iterations: 0.08100561746662185\n",
      "Loss for 146000 iterations: 0.08100561746662184\n",
      "Loss for 147000 iterations: 0.08100561746662183\n",
      "Loss for 148000 iterations: 0.0810056174666218\n",
      "Loss for 149000 iterations: 0.0810056174666218\n",
      "Loss for 150000 iterations: 0.08100561746662178\n",
      "Loss for 151000 iterations: 0.08100561746662178\n",
      "Loss for 152000 iterations: 0.08100561746662177\n",
      "Loss for 153000 iterations: 0.08100561746662177\n",
      "Loss for 154000 iterations: 0.08100561746662177\n",
      "Loss for 155000 iterations: 0.08100561746662177\n",
      "Loss for 156000 iterations: 0.08100561746662177\n",
      "Loss for 157000 iterations: 0.08100561746662177\n",
      "Loss for 158000 iterations: 0.08100561746662177\n",
      "Loss for 159000 iterations: 0.08100561746662177\n",
      "Loss for 160000 iterations: 0.08100561746662177\n",
      "Loss for 161000 iterations: 0.08100561746662177\n",
      "Loss for 162000 iterations: 0.08100561746662177\n",
      "Loss for 163000 iterations: 0.08100561746662177\n",
      "Loss for 164000 iterations: 0.08100561746662177\n",
      "Loss for 165000 iterations: 0.08100561746662177\n",
      "Loss for 166000 iterations: 0.08100561746662177\n",
      "Loss for 167000 iterations: 0.08100561746662177\n",
      "Loss for 168000 iterations: 0.08100561746662177\n",
      "Loss for 169000 iterations: 0.08100561746662177\n",
      "Loss for 170000 iterations: 0.08100561746662177\n",
      "Loss for 171000 iterations: 0.08100561746662177\n",
      "Loss for 172000 iterations: 0.08100561746662176\n",
      "Loss for 173000 iterations: 0.08100561746662177\n",
      "Loss for 174000 iterations: 0.08100561746662177\n",
      "Loss for 175000 iterations: 0.08100561746662176\n",
      "Loss for 176000 iterations: 0.08100561746662177\n",
      "Loss for 177000 iterations: 0.08100561746662177\n",
      "Loss for 178000 iterations: 0.08100561746662177\n",
      "Loss for 179000 iterations: 0.08100561746662176\n",
      "Loss for 180000 iterations: 0.08100561746662177\n",
      "Loss for 181000 iterations: 0.08100561746662177\n",
      "Loss for 182000 iterations: 0.08100561746662177\n",
      "Loss for 183000 iterations: 0.08100561746662176\n",
      "Loss for 184000 iterations: 0.08100561746662176\n",
      "Loss for 185000 iterations: 0.08100561746662177\n",
      "Loss for 186000 iterations: 0.08100561746662177\n",
      "Loss for 187000 iterations: 0.08100561746662177\n",
      "Loss for 188000 iterations: 0.08100561746662177\n",
      "Loss for 189000 iterations: 0.08100561746662177\n",
      "Loss for 190000 iterations: 0.08100561746662177\n",
      "Loss for 191000 iterations: 0.08100561746662176\n",
      "Loss for 192000 iterations: 0.08100561746662176\n",
      "Loss for 193000 iterations: 0.08100561746662177\n",
      "Loss for 194000 iterations: 0.08100561746662177\n",
      "Loss for 195000 iterations: 0.08100561746662177\n",
      "Loss for 196000 iterations: 0.08100561746662177\n",
      "Loss for 197000 iterations: 0.08100561746662176\n",
      "Loss for 198000 iterations: 0.08100561746662177\n",
      "Loss for 199000 iterations: 0.08100561746662177\n",
      "Loss for 200000 iterations: 0.08100561746662177\n",
      "(3,)\n",
      "[[ 0.40588344]\n",
      " [ 0.5916518 ]\n",
      " [ 0.20459822]\n",
      " [ 0.12894294]\n",
      " [ 0.04233999]\n",
      " [ 0.07524285]\n",
      " [ 0.00972053]\n",
      " [-0.32047359]\n",
      " [-0.0769145 ]\n",
      " [-0.06196825]\n",
      " [-0.00197799]\n",
      " [-0.00674367]\n",
      " [-0.00131822]\n",
      " [ 0.05753004]\n",
      " [ 0.01160987]\n",
      " [ 0.00607637]]\n"
     ]
    }
   ],
   "source": [
    "train_datasize = X_train.shape[0]\n",
    "X_mean, X_var, opt_theta, train_loss = polyReg(X_train,y_train,200000,0.01,train_datasize,6)\n",
    "print(X_mean.shape)\n",
    "print(opt_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicter(X_mean, X_var, X_test, opt_theta,degree=2):\n",
    "    X_test_norm = (X_test - X_mean) / X_var\n",
    "    X_test_cross = poly_feat(X_test_norm,degree)\n",
    "    test_size = X_test_cross.shape[0]\n",
    "    X_test_cross = np.append( np.ones((test_size,1)) , X_test_cross , axis=1)\n",
    "\n",
    "    # print(X_test_cross.shape)\n",
    "    # print(opt_theta.shape)\n",
    "    y_pred = np.dot(X_test_cross, opt_theta)\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metrics(X_mean,X_var,X_test,y_test,opt_theta,degree=2):\n",
    "    y_pred = predicter(X_mean, X_var, X_test, opt_theta, degree)\n",
    "    loss_y = y_pred - y_test\n",
    "\n",
    "    test_size = y_pred.shape[0]\n",
    "    total_loss_y = np.dot(np.ones((1,test_size)),np.square(loss_y))[0,0] / test_size\n",
    "\n",
    "    # print(total_loss_y)\n",
    "\n",
    "    y_pred_thresh = y_pred>=0.5\n",
    "\n",
    "    tp = np.sum((y_pred_thresh+y_test)==2 , axis=0)[0]\n",
    "    tn = np.sum(y_pred_thresh==y_test , axis=0)[0] - tp\n",
    "    fp = np.sum(y_pred_thresh , axis=0)[0]-tp\n",
    "    fn = test_size-tp-tn-fp\n",
    "\n",
    "\n",
    "    print('tp: {} , tn: {} , fp: {} , fn: {}'.format(tp,tn,fp,fn))\n",
    "\n",
    "    acc = (tp+tn)/test_size\n",
    "    prec = (tp)/(tp+fp)\n",
    "    recl = (tp)/(tp+fn)\n",
    "    f1 = 2*prec*recl/(prec+recl)\n",
    "\n",
    "    print('Accuracy: {}'.format( acc  ))\n",
    "    print('Precision: {}'.format( prec  ))\n",
    "    print('Recall: {}'.format( recl  ))\n",
    "    print('F1 score: {}'.format( f1  ))\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr=[]\n",
    "te=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Accuracy\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "shapes (490,7) and (16,1) not aligned: 7 (dim 1) != 16 (dim 0)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-66132bc9d00d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_mean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt_theta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..............................................'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-1722ef9851f4>\u001b[0m in \u001b[0;36maccuracy_metrics\u001b[1;34m(X_mean, X_var, X_test, y_test, opt_theta, degree)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maccuracy_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_mean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt_theta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_theta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mloss_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-c6756b76d3a1>\u001b[0m in \u001b[0;36mpredicter\u001b[1;34m(X_mean, X_var, X_test, opt_theta, degree)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# print(X_test_cross.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# print(opt_theta.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_cross\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_theta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (490,7) and (16,1) not aligned: 7 (dim 1) != 16 (dim 0)"
     ]
    }
   ],
   "source": [
    "for d in range(1,10):\n",
    "    print('Train Accuracy')\n",
    "    train_acc = accuracy_metrics(X_mean,X_var,X_train,y_train,opt_theta,degree=d)\n",
    "    print('..............................................')\n",
    "    print('Test Accuracy')\n",
    "    test_acc = accuracy_metrics(X_mean,X_var,X_test,y_test,opt_theta,degree=d)\n",
    "    tr.append(train_acc)\n",
    "    te.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}