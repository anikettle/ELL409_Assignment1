{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(split_percent = 70):\n",
    "    dataset = pd.read_csv('E:\\ELL_project\\problem2\\weather_data.csv')\n",
    "    dataset = dataset.sample(frac = 1)\n",
    "    X = dataset.iloc[:,:-1].values\n",
    "    y = dataset.iloc[:,-1:].values\n",
    "    datasize = X.shape[0]\n",
    "\n",
    "    split_point = split_percent//10\n",
    "\n",
    "    X_train = X[:(datasize*split_point)//10,:]\n",
    "    y_train = y[:(datasize*split_point)//10,:]\n",
    "    X_test = X[(datasize*split_point)//10:,:]\n",
    "    y_test = y[(datasize*split_point)//10:,:]\n",
    "\n",
    "    return(X_train,X_test,y_train,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(X_train,X_test):\n",
    "    X_mean = np.sum(X_train,axis=0)\n",
    "    X_var = np.sqrt(np.sum(np.square(X_train - X_mean), axis=0))\n",
    "\n",
    "    X_train_feat_scaled = (X_train - X_mean ) / X_var\n",
    "    X_test_feat_scaled = (X_test - X_mean) / X_var\n",
    "\n",
    "    return (X_mean,X_var, X_train_feat_scaled,X_test_feat_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradDesc(X,y,theta,hypothesis,loss_function,regularizer):\n",
    "    h,loss,gradient = loss_function(X,theta,y,hypothesis)\n",
    "    reg_loss,reg_grad = regularizer(theta)\n",
    "    loss += reg_loss\n",
    "    gradient += reg_grad\n",
    "\n",
    "    return(loss,gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_regularizer(theta):\n",
    "    a = np.zeros(theta.shape)\n",
    "    return(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_reg(alpha,theta):\n",
    "    reg_loss = alpha*theta\n",
    "    reg_grad = alpha\n",
    "    return(reg_loss,reg_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_reg(alpha,theta):\n",
    "    reg_loss = alpha * np.square(theta)\n",
    "    reg_grad = 2 * alpha * theta\n",
    "    return(reg_loss,reg_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_net_reg(lambda1,lambda2,theta):\n",
    "    a1,a2 = l1_reg(lambda1,theta)\n",
    "    b1,b2 = l2_reg(lambda2,theta)\n",
    "    return( a1+b1 , a2+b2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(X,theta,y,hypothesis):\n",
    "    siz = y.shape[0]\n",
    "    h = hypothesis(X,theta)\n",
    "    diff = h-y\n",
    "    mse = ( np.sum(np.square(diff),axis=0))[0] / siz\n",
    "    gradient = np.dot(np.transpose(X),diff) / siz\n",
    "\n",
    "    return(h,mse,gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_loss(X,theta,y,hypothesis):\n",
    "    siz = y.shape[0]\n",
    "    num_feat = y.shape[1]\n",
    "\n",
    "    h = hypothesis(X,theta)\n",
    "    diff = h-y\n",
    "    diff_sign = np.ones((siz,1))\n",
    "    diff_sign[diff[:,0]<0] = -1\n",
    "\n",
    "    mae = np.sum(np.abs(h),axis=0)[0] / siz\n",
    "    gradient = np.sum(diff_sign*X,axis=0) / siz\n",
    "\n",
    "    return(h,mae,gradient)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(X,theta,y,hypothesis):\n",
    "    siz = y.shape[0]\n",
    "\n",
    "    h = hypothesis(X,theta)\n",
    "    h_comp = 1-h\n",
    "    diff = h - y\n",
    "\n",
    "    ce = np.sum(-y*log(h)-(1-y)*log(1-h),axis=0) [0] / siz\n",
    "    gradient = np.dot(np.transpose(X),diff) / siz\n",
    "\n",
    "    return(h,ce,gradient)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linReg(X,y,iter=100,alpha=0.01,batchSize=32):\n",
    "    # print(X.shape)\n",
    "    theta = np.random.random((X.shape[1],1))\n",
    "    # print(theta)\n",
    "    datasize = X.shape[0]\n",
    "    loss_epoch = 0\n",
    "    for i in range(iter):\n",
    "        if((i+1)%1000==0 and i>0):\n",
    "            print('Loss for {} iterations: {}'.format(i+1,loss_epoch))\n",
    "        fro = 0\n",
    "        loss_epoch = 0\n",
    "        while(True):\n",
    "            to = min(fro+batchSize,datasize)\n",
    "            l,theta_grad = gradDesc(X[fro:to,:],y[fro:to,:],theta)\n",
    "            # print(theta_grad)\n",
    "            loss_epoch += l\n",
    "            theta -= (alpha*theta_grad)\n",
    "            fro = to\n",
    "\n",
    "            if(to>=datasize):\n",
    "                break\n",
    "    \n",
    "    return (theta,loss_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(X_train):\n",
    "    training_size = X_train.shape[0]\n",
    "    X_mean = np.sum(X_train,axis=0) / training_size\n",
    "    X_var = np.sqrt(np.sum((np.square(X-X_mean)),axis=0)/training_size)\n",
    "    X_train_reg = (X_train - X_mean) / X_var\n",
    "    return (X_mean,X_var,X_train_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_feat(X,degree=2):\n",
    "    num_feats = X.shape[1]\n",
    "    num_vals = X.shape[0]\n",
    "    X_cross = X\n",
    "    for i in range(num_feats):\n",
    "        for j in range(i+1,num_feats):\n",
    "            X_cross = np.append( X_cross ,  np.multiply ( X[:,i:i+1] , X[:,j:j+1] ) , axis=1 )\n",
    "    for i in range(3,degree):\n",
    "        X_power = np.power(X,i)\n",
    "        X_cross = np.append(X_cross,X_power,axis=1)\n",
    "\n",
    "    return ( X_cross )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyReg(X,y,iter=100,alpha=0.01,batchSize=32,degree=2):\n",
    "    X_mean,X_var,X_norm = feature_scaling(X)\n",
    "    X_cross = poly_feat(X_norm,degree)\n",
    "    train_size = X_cross.shape[0]\n",
    "    X_cross = np.append(np.ones((train_size,1)),X_cross,axis=1)\n",
    "    opt, tl = linReg(X_cross,y,iter=iter,alpha=alpha,batchSize=batchSize)\n",
    "    return ( X_mean, X_var, opt , tl )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_datasize = X_train.shape[0]\n",
    "X_mean, X_var, opt_theta, train_loss = polyReg(X_train,y_train,200000,0.01,train_datasize,6)\n",
    "print(X_mean.shape)\n",
    "print(opt_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicter(X_mean, X_var, X_test, opt_theta,degree=2):\n",
    "    X_test_norm = (X_test - X_mean) / X_var\n",
    "    X_test_cross = poly_feat(X_test_norm,degree)\n",
    "    test_size = X_test_cross.shape[0]\n",
    "    X_test_cross = np.append( np.ones((test_size,1)) , X_test_cross , axis=1)\n",
    "\n",
    "    # print(X_test_cross.shape)\n",
    "    # print(opt_theta.shape)\n",
    "    y_pred = np.dot(X_test_cross, opt_theta)\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metrics(X_mean,X_var,X_test,y_test,opt_theta,degree=2):\n",
    "    y_pred = predicter(X_mean, X_var, X_test, opt_theta, degree)\n",
    "    loss_y = y_pred - y_test\n",
    "\n",
    "    test_size = y_pred.shape[0]\n",
    "    total_loss_y = np.dot(np.ones((1,test_size)),np.square(loss_y))[0,0] / test_size\n",
    "\n",
    "    # print(total_loss_y)\n",
    "\n",
    "    y_pred_thresh = y_pred>=0.5\n",
    "\n",
    "    tp = np.sum((y_pred_thresh+y_test)==2 , axis=0)[0]\n",
    "    tn = np.sum(y_pred_thresh==y_test , axis=0)[0] - tp\n",
    "    fp = np.sum(y_pred_thresh , axis=0)[0]-tp\n",
    "    fn = test_size-tp-tn-fp\n",
    "\n",
    "\n",
    "    print('tp: {} , tn: {} , fp: {} , fn: {}'.format(tp,tn,fp,fn))\n",
    "\n",
    "    acc = (tp+tn)/test_size\n",
    "    prec = (tp)/(tp+fp)\n",
    "    recl = (tp)/(tp+fn)\n",
    "    f1 = 2*prec*recl/(prec+recl)\n",
    "\n",
    "    print('Accuracy: {}'.format( acc  ))\n",
    "    print('Precision: {}'.format( prec  ))\n",
    "    print('Recall: {}'.format( recl  ))\n",
    "    print('F1 score: {}'.format( f1  ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Accuracy')\n",
    "\n",
    "accuracy_metrics(X_mean,X_var,X_train,y_train,opt_theta,degree=6)\n",
    "print('..............................................')\n",
    "print('Test Accuracy')\n",
    "accuracy_metrics(X_mean,X_var,X_test,y_test,opt_theta,degree=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}