{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import decomposition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_list_of_data(bpt = 2000):\n",
    "    data_dir = 'E:\\ELL_Project\\problem3\\\\'\n",
    "    lis_folders = os.listdir(data_dir+'.')[:-1]\n",
    "    X = []\n",
    "    cur_class = 0\n",
    "    for folder in lis_folders:\n",
    "        i=0\n",
    "        lis_files = os.listdir(data_dir+folder+'\\.')\n",
    "        im = []\n",
    "        for files in lis_files:\n",
    "            i+=1\n",
    "            if(i==bpt+1):\n",
    "                break\n",
    "            img = '\\\\'.join([data_dir,folder,files])\n",
    "            image = plt.imread(img)\n",
    "            im.append(image.flatten().reshape(-1,1))\n",
    "        im_np = np.array(im)[:,:,0]\n",
    "        im_np = np.append(im_np,cur_class*np.ones((im_np.shape[0],1)),axis=1)\n",
    "        cur_class+=1\n",
    "        X.append(im_np)\n",
    "    return X\n",
    "\n",
    "def dimensional_shrinking_pca(X,n_components=2):\n",
    "    pca = decomposition.PCA(n_components)\n",
    "    X_train = X[0]\n",
    "    for x in X[1:]: \n",
    "        X_train = np.append(X_train,x,axis=0)\n",
    "    X_pca = X_train\n",
    "    np.random.shuffle(X_pca)\n",
    "    pca.fit(X_pca[:,:-1])\n",
    "    X_pca_ret = np.zeros((X_train.shape[0],n_components))\n",
    "    y_pca_ret = X_pca[:,-1].reshape(-1,1)\n",
    "    X_pca_ret = pca.transform(X_pca[:,:-1])\n",
    "\n",
    "    return (X_pca_ret,y_pca_ret)\n",
    "\n",
    "def one_hot(y,num_feats):\n",
    "    y = y.reshape(-1,1)\n",
    "    print(y)\n",
    "    ret = np.zeros((y.shape[0],num_feats))\n",
    "    rows = np.arange(y.shape[0])\n",
    "    # print(y)\n",
    "    # print(rows)\n",
    "    ret[ rows , y[:,0].astype(int)] = 1\n",
    "    return ret\n",
    "\n",
    "def import_dataset(X,y,split_percent = 70):\n",
    "    \n",
    "    datasize = X.shape[0]\n",
    "\n",
    "    split_point = split_percent//10\n",
    "\n",
    "    X_train = X[:(datasize*split_point)//10,:]\n",
    "    y_train = y[:(datasize*split_point)//10,:]\n",
    "    X_test = X[(datasize*split_point)//10:,:]\n",
    "    y_test = y[(datasize*split_point)//10:,:]\n",
    "\n",
    "    return(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_hyp(X,theta):\n",
    "    return (np.dot(X,theta))\n",
    "def null_regularizer(alpha,theta):\n",
    "    a = np.zeros(theta.shape)\n",
    "    return(a,a)\n",
    "def l1_reg(alpha,theta):\n",
    "    reg_loss = alpha*theta\n",
    "    reg_grad = alpha\n",
    "    return(reg_loss,reg_grad)\n",
    "def l2_reg(alpha,theta):\n",
    "    reg_loss = alpha * np.square(theta)\n",
    "    reg_grad = 2 * alpha * theta\n",
    "    return(reg_loss,reg_grad)\n",
    "def elastic_net_reg(lambda1,theta,lambda2=0):\n",
    "    if(lambda2==0):\n",
    "        lambda2=lambda1\n",
    "    a1,a2 = l1_reg(lambda1,theta)\n",
    "    b1,b2 = l2_reg(lambda2,theta)\n",
    "    return( a1+b1 , a2+b2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(X,theta,y,hypothesis):\n",
    "    siz = y.shape[0]\n",
    "    h = hypothesis(X,theta)\n",
    "    diff = h-y\n",
    "    mse = ( np.sum(np.square(diff),axis=0))[0] / siz\n",
    "    gradient = np.dot(np.transpose(X),diff) / siz\n",
    "    return(h,mse,gradient)\n",
    "\n",
    "def mae_loss(X,theta,y,hypothesis):\n",
    "    siz = y.shape[0]\n",
    "    num_feat = y.shape[1]\n",
    "    h = hypothesis(X,theta)\n",
    "    diff = h-y\n",
    "    diff_sign = np.ones((siz,1))\n",
    "    diff_sign[diff[:,0]<0] = -1\n",
    "    mae = np.sum(np.abs(h),axis=0)[0] / siz\n",
    "    gradient = np.sum(diff_sign*X,axis=0) / siz\n",
    "    return(h,mae,gradient)\n",
    "\n",
    "def ce_loss(X,theta,y,hypothesis):\n",
    "    siz = y.shape[0]\n",
    "    h = hypothesis(X,theta)\n",
    "    h_comp = 1-h\n",
    "    diff = h - y\n",
    "    ce = np.sum(-y*log(h)-(1-y)*log(1-h),axis=0) [0] / siz\n",
    "    gradient = np.dot(np.transpose(X),diff) / siz\n",
    "    return(h,ce,gradient)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradDesc(X,y,theta,hypothesis,loss_function,regularizer,alpha):\n",
    "    h,loss,gradient = loss_function(X,theta,y,hypothesis)\n",
    "    reg_loss,reg_grad = regularizer(alpha,theta)\n",
    "    loss += reg_loss\n",
    "    gradient += reg_grad\n",
    "\n",
    "    return(loss,gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg(X,y,iter=1000,alpha=0.01,batchSize=32,n_components=2,classes=6):\n",
    "    (X_mean,X_var,X) = feature_scaling(X)\n",
    "    X = np.append(np.ones((X.shape[0],1)),X,axis=1)\n",
    "    theta = np.random.random((X.shape[1],classes))\n",
    "    datasize = X.shape[0]\n",
    "    for cl in range(classes):\n",
    "        y_class = y[:,cl:cl+1]\n",
    "        loss_epoch,_ = gradDesc(X,y_class,theta[:,cl:cl+1],lin_hyp,mse_loss,l2_reg,alpha)\n",
    "        for i in range(iter+1):\n",
    "            if((i)%100000==0):\n",
    "                print('Loss for {} iterations: {}'.format(i,np.sum(loss_epoch,axis=0)[0]))\n",
    "            fro = 0 \n",
    "            loss_epoch = 0\n",
    "            while(True):\n",
    "                to = min(fro+batchSize,datasize)\n",
    "                l,theta_grad = gradDesc(X[fro:to,:],y_class[fro:to,:],theta[:,cl:cl+1],lin_hyp,mse_loss,l2_reg,alpha)\n",
    "                loss_epoch += l\n",
    "                theta[:,cl:cl+1] -= (alpha*theta_grad)\n",
    "                fro = to\n",
    "\n",
    "                if(to>=datasize):\n",
    "                    break\n",
    "    return (X_mean,X_var,theta,loss_epoch)\n",
    "def sigmoid(X):\n",
    "    ret = 1/(1+np.exp(-X))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(X_train):\n",
    "    training_size = X_train.shape[0]\n",
    "    X_mean = np.sum(X_train,axis=0) / training_size\n",
    "    X_var = np.sqrt(np.sum((np.square(X_train-X_mean)),axis=0)/training_size)\n",
    "    X_train_reg = (X_train - X_mean) / X_var\n",
    "    return (X_mean,X_var,X_train_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicter(X_mean, X_var, X_test, opt_theta):\n",
    "    X_test_norm = (X_test - X_mean) / X_var\n",
    "    X_test_cross = X_test_norm\n",
    "    test_size = X_test_cross.shape[0]\n",
    "    X_test_cross = np.append( np.ones((test_size,1)) , X_test_cross , axis=1)\n",
    "    y_pred = np.dot(X_test_cross, opt_theta)\n",
    "    y_pred = sigmoid(y_pred)\n",
    "    y_pred_norm = y_pred / (np.sum(y_pred,axis=1).reshape(-1,1))\n",
    "    y_pred_class = y_pred.argmax(1)\n",
    "    return y_pred_class,y_pred_norm\n",
    "\n",
    "def acc(X_mean,X_var,X_test,y_test,opt_theta,n_classes=6):\n",
    "    y_pred_class,y_pred_norm = predicter(X_mean, X_var, X_test, opt_theta)\n",
    "    y_pred_class=y_pred_class.reshape(-1,1)\n",
    "    y_test_one_hot = one_hot(y_test,n_classes)\n",
    "    y_pred_one_hot = one_hot(y_pred_class,n_classes)\n",
    "    print(y_test.shape)\n",
    "    print(y_pred_class.shape)\n",
    "    print(y_test[y_test[:,0]==y_pred_class[:,0],0].shape)\n",
    "    print('OVERALL ACCURACY = {}'.format(y_test[y_test[:]==y_pred_class[:]].shape[0]))\n",
    "    for i in range(n_classes):\n",
    "        print('class {} .......................................'.format(i))\n",
    "        accuracy_metrics_classif(y_pred_one_hot[:,i:i+1],y_test_one_hot[:,i:i+1])\n",
    "\n",
    "def accuracy_metrics_classif(y_pred,y_test):\n",
    "    \n",
    "    loss_y = y_pred - y_test\n",
    "    test_size = y_pred.shape[0]\n",
    "    total_loss_y = np.dot(np.ones((1,test_size)),np.square(loss_y))[0,0] / test_size\n",
    "    y_pred_thresh = y_pred>=0.5\n",
    "\n",
    "    tp = np.sum((y_pred_thresh+y_test)==2 , axis=0)[0]\n",
    "    tn = np.sum(y_pred_thresh==y_test , axis=0)[0] - tp\n",
    "    fp = np.sum(y_pred_thresh , axis=0)[0]-tp\n",
    "    fn = test_size-tp-tn-fp\n",
    "\n",
    "    print('tp: {} , tn: {} , fp: {} , fn: {}'.format(tp,tn,fp,fn))\n",
    "    acc = (tp+tn)/test_size\n",
    "    prec = (tp)/(tp+fp)\n",
    "    recl = (tp)/(tp+fn)\n",
    "    f1 = 2*prec*recl/(prec+recl)\n",
    "    print('Accuracy: {}'.format( acc  ))\n",
    "    print('Precision: {}'.format( prec  ))\n",
    "    print('Recall: {}'.format( recl  ))\n",
    "    print('F1 score: {}'.format( f1  ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_run(X_train,X_test,y_train,y_test,degree = 3, plot = False,alpha=0.0001,n_classes = 6):\n",
    "    # X_mean,X_var, X_train_feat_scaled = feature_scaling(X_train , X_test)\n",
    "    y_train_one_hot = one_hot(y_train,n_classes)\n",
    "    y_test_one_hot = one_hot(y_test,n_classes)\n",
    "    train_datasize = X_train.shape[0]\n",
    "    X_mean, X_var, opt_theta, train_loss = logreg(X_train,y_train_one_hot,50000,alpha,train_datasize,degree,n_classes)\n",
    "    y_pred_class,y_pred_norm = predicter(X_mean, X_var, X_test, opt_theta)\n",
    "    acc(X_mean,X_var,X_test,y_test,opt_theta)\n",
    "\n",
    "    return(X_mean, X_var,opt_theta, degree,y_pred)\n",
    "# print_accuracy_metric_classif(X_mean,X_var,X_train,X_test,y_train,opt_theta,degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "n_classes = 6\n",
    "data = return_list_of_data(bpt = 1000)\n",
    "X_pca_ret,y_pca_ret = dimensional_shrinking_pca(data,n_components)\n",
    "X_train,X_test,y_train,y_test = import_dataset(X_pca_ret,y_pca_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1.]\n",
      " [3.]\n",
      " [3.]\n",
      " ...\n",
      " [3.]\n",
      " [5.]\n",
      " [1.]]\n",
      "[[1.]\n",
      " [2.]\n",
      " [3.]\n",
      " ...\n",
      " [2.]\n",
      " [5.]\n",
      " [1.]]\n",
      "Loss for 0 iterations: 2.1291350733539534\n",
      "Loss for 0 iterations: 4.046016816906684\n",
      "Loss for 0 iterations: 2.2472293475704808\n",
      "Loss for 0 iterations: 3.3995341057385655\n",
      "Loss for 0 iterations: 2.4648503465528404\n",
      "Loss for 0 iterations: 4.1948049730353105\n",
      "[[1.]\n",
      " [2.]\n",
      " [3.]\n",
      " ...\n",
      " [2.]\n",
      " [5.]\n",
      " [1.]]\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " ...\n",
      " [2]\n",
      " [4]\n",
      " [1]]\n",
      "(1800, 1)\n",
      "(1800, 1)\n",
      "(1169,)\n",
      "OVERALL ACCURACY = 1169\n",
      "class 0 .......................................\n",
      "tp: 0 , tn: 1492 , fp: 0 , fn: 308\n",
      "Accuracy: 0.8288888888888889\n",
      "Precision: nan\n",
      "Recall: 0.0\n",
      "F1 score: nan\n",
      "class 1 .......................................\n",
      "tp: 311 , tn: 1283 , fp: 206 , fn: 0\n",
      "Accuracy: 0.8855555555555555\n",
      "Precision: 0.6015473887814313\n",
      "Recall: 1.0\n",
      "F1 score: 0.751207729468599\n",
      "class 2 .......................................\n",
      "tp: 299 , tn: 1189 , fp: 312 , fn: 0\n",
      "Accuracy: 0.8266666666666667\n",
      "Precision: 0.48936170212765956\n",
      "Recall: 1.0\n",
      "F1 score: 0.6571428571428571\n",
      "class 3 .......................................\n",
      "tp: 291 , tn: 1482 , fp: 15 , fn: 12\n",
      "Accuracy: 0.985\n",
      "Precision: 0.9509803921568627\n",
      "Recall: 0.9603960396039604\n",
      "F1 score: 0.9556650246305418\n",
      "class 4 .......................................\n",
      "tp: 268 , tn: 1412 , fp: 98 , fn: 22\n",
      "Accuracy: 0.9333333333333333\n",
      "Precision: 0.73224043715847\n",
      "Recall: 0.9241379310344827\n",
      "F1 score: 0.8170731707317074\n",
      "class 5 .......................................\n",
      "tp: 0 , tn: 1511 , fp: 0 , fn: 289\n",
      "Accuracy: 0.8394444444444444\n",
      "Precision: nan\n",
      "Recall: 0.0\n",
      "F1 score: nan\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-ea8fd3ee57ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt_theta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-120-cc780ad89080>\u001b[0m in \u001b[0;36mmodel_run\u001b[1;34m(X_train, X_test, y_train, y_test, degree, plot, alpha, n_classes)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0macc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_mean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt_theta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt_theta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m# print_accuracy_metric_classif(X_mean,X_var,X_train,X_test,y_train,opt_theta,degree=2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "X_mean, X_var,opt_theta, degree,y_pred = model_run(X_train,X_test,y_train,y_test,power,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1 2]\n [2 2]]\n"
     ]
    }
   ],
   "source": [
    "# a = np.array([1,2,2,2])\n",
    "# print(a.reshape(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}